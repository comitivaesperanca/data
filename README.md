# üé≤ Data
Reposit√≥rio para armazenar todos os componentes referentes as atividades de Machine Learning do projeto [pantanal.dev](<Pantanal.dev>) da [Comitiva Esperan√ßa](<https://github.com/comitivaesperanca>).

## üÜò Como executar a plataforma de Machine Learning?
A plataforma √© composta por 2 componentes principais:
- [Apache Airflow](<https://airflow.apache.org/>)
- [Jupyter](<https://jupyter.org/>)

Para executar a plataforma, √© necess√°rio ter o [Docker](<https://www.docker.com/>) instalado na m√°quina. <br>

Inicialmente, clone o reposit√≥rio para sua m√°quina, seguindo os passos abaixos:
```bash
git clone
```
Em seguida, execute o comando:
```bash
docker-compose up -d --build
```

Ap√≥s a execu√ß√£o do comando, a plataforma estar√° dispon√≠vel para uso. <br>
- Para acessar o *Apache Airflow*, acesse o endere√ßo: http://localhost:8080
- Para acessar o *Jupyter*, acesse o endere√ßo: http://localhost:8888

## üíª Tecnologias

### Docker
O Docker foi utilizado no projeto com o objetivo de facilitar a execu√ß√£o da plataforma de Machine Learning. <br>
Com o Docker, √© poss√≠vel executar a plataforma em qualquer sistema operacional, sem a necessidade de instalar as depend√™ncias necess√°rias para a execu√ß√£o do projeto. <br>

### Apache Airflow
Apache Airflow √© uma plataforma para criar, programar e monitorar fluxos de trabalho. √â uma ferramenta de c√≥digo aberto para automatizar fluxos de trabalho complexos e gerenciar tarefas de forma program√°tica.
No projeto, ser√° utilizada para orquestrar o scrapping de dados e o treinamento dos modelos de Machine Learning. <br>
No Airflow, cada fluxo √© chamado de Dag (Directed Acyclic Graph). Uma Dag √© composta por tarefas, que s√£o executadas em sequ√™ncia ou em paralelo. Cada tarefa √© executada por um operador, que √© respons√°vel por executar a tarefa. Os operadores s√£o respons√°veis por executar as tarefas, que podem ser qualquer coisa, desde um comando bash at√© um script Python.
#### Como criar novas tarefas?
Na pasta Dags do Airflow, h√° duas subpastas: <br>
- **data_engineering**: cont√©m os arquivos para scrapping de textos e dados 
- **data_science**: cont√©m os arquivos para treinamento dos modelos de Machine Learning

Dentro de cada uma das pastas, h√° um arquivo chamado **dag.py**. Esse arquivo √© respons√°vel por criar a Dag e adicionar as tarefas. <br>
Para criar uma nova tarefa, basta criar um arquivo em Python (Ex. task_example.py) no diret√≥rio **common/tasks**, como por exemplo:
```python
def task_example():
    print("Hello World!")
```
Em seguida, importar o arquivo no arquivo **dag.py** e adicionar a tarefa na Dag:
```python

from common.tasks.task_example import task_example
task_example = PythonOperator(
    task_id='task_example',
    python_callable=task_example,
    dag=dag
)
```
Em seguida, adicionar a tarefa na sequ√™ncia de execu√ß√£o da Dag:
```python
task_example >> task_2
```

A partir disso, sua task ser√° exibida na lista de Dags do Airflow.



### Jupyter 
Jupyter √© uma aplica√ß√£o web que permite criar e compartilhar documentos que cont√©m c√≥digo, equa√ß√µes, visualiza√ß√µes e texto explicativo. Os documentos Jupyter s√£o chamados de notebooks. Os notebooks s√£o executados em um servidor e s√£o acessados por meio de um navegador web. O Jupyter Notebook √© uma aplica√ß√£o web de c√≥digo aberto que permite criar e compartilhar documentos que cont√©m c√≥digo, equa√ß√µes, visualiza√ß√µes e texto explicativo. Os documentos Jupyter s√£o chamados de notebooks. Os notebooks s√£o executados em um servidor e s√£o acessados por meio de um navegador web.

#### Como criar novos notebooks?
Basta abrir o Jupyter atrav√©s do link http://localhost:8888 e criar um novo notebook. <br>
Para criar um novo notebook, basta clicar no bot√£o **New** e selecionar a op√ß√£o **Python 3**.

#### Como instalar novas bibliotecas?
Para instalar novas bibliotecas, basta adicionar no arquivo **requirements.txt** e executar o comando, em seu terminal:
```bash
docker-compose up -d --build
```