{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Npl3heW12Cn",
        "outputId": "77697fe0-bd89-4094-8d05-995bcbeaf9df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.0.0+cu118)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (6.0.10)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.11.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.9.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.5.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (3.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.2.2)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.3.6)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (4.29.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (0.4.8)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (0.32.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 2)) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 2)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 2)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 2)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 2)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 2)) (16.0.3)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser->-r requirements.txt (line 3)) (1.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->-r requirements.txt (line 4)) (2.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 6)) (2022.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 7)) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 7)) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 7)) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 7)) (4.65.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 8)) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 8)) (3.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 10)) (0.14.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 10)) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 10)) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 10)) (0.13.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 1)) (0.40.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers->-r requirements.txt (line 10)) (2023.4.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow->-r requirements.txt (line 1)) (0.1.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 1)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 1)) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 1)) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 10)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 10)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 10)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 10)) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 2)) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 1)) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 1)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 1)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 1)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->-r requirements.txt (line 1)) (3.2.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "<ipython-input-10-dd8b04155bbc>:33: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  df = pd.concat([df.drop(['result'], axis=1), df['result'].apply(pd.Series)], axis=1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "file_upload                 421\n",
              "drafts                      421\n",
              "predictions                 421\n",
              "meta                        421\n",
              "created_at                  421\n",
              "updated_at                  421\n",
              "inner_id                    421\n",
              "total_annotations           421\n",
              "cancelled_annotations       421\n",
              "total_predictions           421\n",
              "comment_count               421\n",
              "unresolved_comment_count    421\n",
              "last_comment_updated_at       0\n",
              "project                     421\n",
              "updated_by                  421\n",
              "comment_authors             421\n",
              "completed_by                421\n",
              "was_cancelled               421\n",
              "ground_truth                421\n",
              "created_at                  421\n",
              "updated_at                  421\n",
              "lead_time                   421\n",
              "prediction                  421\n",
              "result_count                421\n",
              "unique_id                   421\n",
              "last_action                   0\n",
              "task                        421\n",
              "project                     421\n",
              "updated_by                  421\n",
              "parent_prediction             0\n",
              "parent_annotation             0\n",
              "last_created_by               0\n",
              "id                          421\n",
              "type                        421\n",
              "origin                      421\n",
              "to_name                     421\n",
              "from_name                   421\n",
              "0                             0\n",
              "choices                     421\n",
              "0                             0\n",
              "ano                         421\n",
              "mes                         421\n",
              "url                         421\n",
              "data                        421\n",
              "titulo                      421\n",
              "noticia                     421\n",
              "data_hora                   421\n",
              "Unnamed: 0                  421\n",
              "qtde_palavras_soja          421\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "!pip install -r requirements.txt\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from unidecode import unidecode\n",
        "## ler arquivo json em pandas\n",
        "df = pd.read_json('project-1-at-2023-05-11-23-47-a0f8a77e.json')\n",
        "\n",
        "df = df.drop(['id'], axis=1)\n",
        "## expandir coluna annotations\n",
        "df = pd.concat([df.drop(['annotations'], axis=1), df['annotations'].apply(pd.Series)], axis=1)\n",
        "## expandir coluna 0 e renomear para annotations\n",
        "df = pd.concat([df.drop([0], axis=1), df[0].apply(pd.Series)], axis=1)\n",
        "## expandir coluna result e renomear para result\n",
        "df = pd.concat([df.drop(['result'], axis=1), df['result'].apply(pd.Series)], axis=1)\n",
        "df = df.drop(['id'], axis=1)\n",
        "\n",
        "## expandir coluna 0 e renomear para result\n",
        "df = pd.concat([df.drop([0], axis=1), df[0].apply(pd.Series)], axis=1)\n",
        "## expandir coluna value e renomear para value\n",
        "df = pd.concat([df.drop(['value'], axis=1), df['value'].apply(pd.Series)], axis=1)\n",
        "## dropar choices nulos\n",
        "df = df.dropna(subset=['choices'])\n",
        "## obter choices \n",
        "df['choices'] = df['choices'].apply(lambda x: x[0])\n",
        "\n",
        "## expandir coluna data\n",
        "df = pd.concat([df.drop(['data'], axis=1), df['data'].apply(pd.Series)], axis=1)\n",
        "\n",
        "df_noticia_original = df.copy()\n",
        "\n",
        "\n",
        "padrao_data_cepea = r\"Cepea, \\d{2}/\\d{2}/\\d{4} - \"\n",
        "df['noticia'] = df['noticia'].apply(lambda x: re.sub(padrao_data_cepea, '', x))\n",
        "\n",
        "## remover a palavra 'cepea' das noticias\n",
        "padrao_cepea = r\"Cepea\"\n",
        "df['noticia'] = df['noticia'].apply(lambda x: re.sub(padrao_cepea, '', x, flags=re.IGNORECASE))\n",
        "\n",
        "## remover numeros das noticias\n",
        "padrao_numeros = r'[0-9]+'\n",
        "df['noticia'] = df['noticia'].apply(lambda x: re.sub(padrao_numeros, '', x))\n",
        "\n",
        "## noticia que contem a palavra 'soja'\n",
        "df = df[df['titulo'].str.contains('soja', flags=re.IGNORECASE)]\n",
        "\n",
        "## remover noticias com choice 'desclassificar'\n",
        "df = df[df['choices'] != 'Desclassificar']\n",
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9RizkRq12Cp"
      },
      "outputs": [],
      "source": [
        "# Selecionar apenas as colunas necessárias\n",
        "columns_to_select = ['id', 'data', 'noticia', 'titulo', 'choices', 'unique_id']\n",
        "\n",
        "df = df[columns_to_select]\n",
        "df.dropna(subset=['noticia'])\n",
        "\n",
        "# Pré-processamento dos dados\n",
        "stop_words = set(stopwords.words('portuguese'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # remover acentuação\n",
        "    text = unidecode(text)\n",
        "    # Remover pontuações\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Tokenização\n",
        "    words = word_tokenize(text.lower())\n",
        "    # Remover stopwords\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "#df['noticia'] = df['noticia'].apply(preprocess_text)\n",
        "df_treino = df[:336]\n",
        "df_validacao = df[336:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYCSlSTS12Cq",
        "outputId": "cfeffceb-5b70-4c22-b0f5-73fc21f48892"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(29794, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer  # Or BertTokenizer\n",
        "from transformers import AutoModelForPreTraining  # Or BertForPreTraining for loading pretraining heads\n",
        "from transformers import AutoModel  # or BertModel, for BERT without pretraining heads\n",
        "import numpy as np\n",
        "\n",
        "from typing import List, Optional, Tuple, Union\n",
        "\n",
        "# Carregar o modelo pré-treinado BERTimbau\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', num_labels=3)\n",
        "model = BertModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "modelo_soja = model\n",
        "\n",
        "modelo_soja"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3O3LnH9g6x64",
        "outputId": "dd31bfaa-6ad4-4870-dd65-abe39c55b984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5lDdPPI12Cr"
      },
      "outputs": [],
      "source": [
        "class LIABertClassifier(nn.Module):\n",
        "    def __init__(self,model,num_labels):\n",
        "        super(LIABertClassifier,self).__init__()\n",
        "        self.bert = model\n",
        "        self.config = model.config\n",
        "        self.num_labels = num_labels\n",
        "        self.cls = nn.Linear(self.config.hidden_size,200)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.cls2 = nn.Linear(768,num_labels)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        token_type_ids: Optional[torch.Tensor] = None,\n",
        "        ) ->Tuple[torch.Tensor]:\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0][:,0,:]\n",
        "        prediction = self.dropout(sequence_output)\n",
        "        prediction = self.cls2(prediction)\n",
        "        return prediction\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bcRVbri12Cr",
        "outputId": "83de41f3-5abe-4212-bb92-efe513bccf43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss=1.0939, LR=0.000050\n",
            "Epoch 0, Loss=0.9335, LR=0.000050\n",
            "Epoch 0, Loss=1.4008, LR=0.000050\n",
            "Epoch 0, Loss=1.0125, LR=0.000050\n",
            "Epoch 0, Loss=1.1969, LR=0.000050\n",
            "Epoch 0, Loss=1.0570, LR=0.000050\n",
            "Epoch 0, Loss=0.8592, LR=0.000049\n",
            "Epoch 0, Loss=1.2487, LR=0.000049\n",
            "Epoch 0, Loss=0.9386, LR=0.000049\n",
            "Epoch 0, Loss=1.2180, LR=0.000049\n",
            "Epoch 0, Loss=1.1786, LR=0.000049\n",
            "Epoch 0, Loss=1.0961, LR=0.000049\n",
            "Epoch 0, Loss=1.1410, LR=0.000049\n",
            "Epoch 0, Loss=1.0289, LR=0.000049\n",
            "Epoch 0, Loss=1.1304, LR=0.000049\n",
            "Epoch 0, Loss=1.1338, LR=0.000049\n",
            "Epoch 0, Loss=1.2214, LR=0.000049\n",
            "Epoch 0, Loss=0.8935, LR=0.000048\n",
            "Epoch 0, Loss=1.1160, LR=0.000048\n",
            "Epoch 0, Loss=1.0662, LR=0.000048\n",
            "Epoch 0, Loss=1.1953, LR=0.000048\n",
            "Epoch 0, Loss=1.1424, LR=0.000048\n",
            "Epoch 0, Loss=1.0490, LR=0.000048\n",
            "Epoch 0, Loss=1.0104, LR=0.000048\n",
            "Epoch 0, Loss=1.0263, LR=0.000048\n",
            "Epoch 0, Loss=1.2263, LR=0.000048\n",
            "Epoch 0, Loss=1.2370, LR=0.000048\n",
            "Epoch 0, Loss=1.0619, LR=0.000048\n",
            "Epoch 0, Loss=1.2064, LR=0.000048\n",
            "Epoch 0, Loss=1.0306, LR=0.000048\n",
            "Epoch 1, Loss=0.9799, LR=0.000047\n",
            "Epoch 1, Loss=0.9340, LR=0.000047\n",
            "Epoch 1, Loss=1.0673, LR=0.000047\n",
            "Epoch 1, Loss=0.9068, LR=0.000047\n",
            "Epoch 1, Loss=1.1180, LR=0.000047\n",
            "Epoch 1, Loss=1.1019, LR=0.000047\n",
            "Epoch 1, Loss=1.1268, LR=0.000047\n",
            "Epoch 1, Loss=1.1628, LR=0.000047\n",
            "Epoch 1, Loss=0.8137, LR=0.000047\n",
            "Epoch 1, Loss=1.1913, LR=0.000047\n",
            "Epoch 1, Loss=1.0775, LR=0.000047\n",
            "Epoch 1, Loss=1.4190, LR=0.000047\n",
            "Epoch 1, Loss=1.0891, LR=0.000046\n",
            "Epoch 1, Loss=0.9311, LR=0.000046\n",
            "Epoch 1, Loss=1.0251, LR=0.000046\n",
            "Epoch 1, Loss=0.9681, LR=0.000046\n",
            "Epoch 1, Loss=1.1480, LR=0.000046\n",
            "Epoch 1, Loss=0.8835, LR=0.000046\n",
            "Epoch 1, Loss=1.1087, LR=0.000046\n",
            "Epoch 1, Loss=1.1857, LR=0.000046\n",
            "Epoch 1, Loss=1.1252, LR=0.000046\n",
            "Epoch 1, Loss=0.8652, LR=0.000046\n",
            "Epoch 1, Loss=0.9309, LR=0.000046\n",
            "Epoch 1, Loss=1.0784, LR=0.000046\n",
            "Epoch 1, Loss=0.9771, LR=0.000045\n",
            "Epoch 1, Loss=1.1209, LR=0.000045\n",
            "Epoch 1, Loss=1.1332, LR=0.000045\n",
            "Epoch 1, Loss=1.0456, LR=0.000045\n",
            "Epoch 1, Loss=1.1760, LR=0.000045\n",
            "Epoch 1, Loss=1.0127, LR=0.000045\n",
            "Epoch 2, Loss=0.7496, LR=0.000045\n",
            "Epoch 2, Loss=0.9383, LR=0.000045\n",
            "Epoch 2, Loss=1.0266, LR=0.000045\n",
            "Epoch 2, Loss=0.7533, LR=0.000045\n",
            "Epoch 2, Loss=0.8411, LR=0.000045\n",
            "Epoch 2, Loss=1.0381, LR=0.000045\n",
            "Epoch 2, Loss=0.6725, LR=0.000044\n",
            "Epoch 2, Loss=0.9812, LR=0.000044\n",
            "Epoch 2, Loss=0.8083, LR=0.000044\n",
            "Epoch 2, Loss=0.9656, LR=0.000044\n",
            "Epoch 2, Loss=1.0563, LR=0.000044\n",
            "Epoch 2, Loss=0.7274, LR=0.000044\n",
            "Epoch 2, Loss=0.5955, LR=0.000044\n",
            "Epoch 2, Loss=0.6903, LR=0.000044\n",
            "Epoch 2, Loss=0.4880, LR=0.000044\n",
            "Epoch 2, Loss=0.5961, LR=0.000044\n",
            "Epoch 2, Loss=0.4952, LR=0.000044\n",
            "Epoch 2, Loss=1.1182, LR=0.000044\n",
            "Epoch 2, Loss=1.3155, LR=0.000043\n",
            "Epoch 2, Loss=0.7213, LR=0.000043\n",
            "Epoch 2, Loss=1.3214, LR=0.000043\n",
            "Epoch 2, Loss=0.3422, LR=0.000043\n",
            "Epoch 2, Loss=0.6016, LR=0.000043\n",
            "Epoch 2, Loss=0.3524, LR=0.000043\n",
            "Epoch 2, Loss=0.8071, LR=0.000043\n",
            "Epoch 2, Loss=0.5955, LR=0.000043\n",
            "Epoch 2, Loss=1.1158, LR=0.000043\n",
            "Epoch 2, Loss=0.8712, LR=0.000043\n",
            "Epoch 2, Loss=0.7048, LR=0.000043\n",
            "Epoch 2, Loss=0.2182, LR=0.000043\n",
            "Epoch 3, Loss=0.2007, LR=0.000042\n",
            "Epoch 3, Loss=0.5182, LR=0.000042\n",
            "Epoch 3, Loss=0.7027, LR=0.000042\n",
            "Epoch 3, Loss=0.4417, LR=0.000042\n",
            "Epoch 3, Loss=0.5333, LR=0.000042\n",
            "Epoch 3, Loss=0.6251, LR=0.000042\n",
            "Epoch 3, Loss=0.4099, LR=0.000042\n",
            "Epoch 3, Loss=0.5969, LR=0.000042\n",
            "Epoch 3, Loss=0.3873, LR=0.000042\n",
            "Epoch 3, Loss=0.7557, LR=0.000042\n",
            "Epoch 3, Loss=0.5996, LR=0.000042\n",
            "Epoch 3, Loss=0.6170, LR=0.000041\n",
            "Epoch 3, Loss=0.5567, LR=0.000041\n",
            "Epoch 3, Loss=0.2361, LR=0.000041\n",
            "Epoch 3, Loss=1.5180, LR=0.000041\n",
            "Epoch 3, Loss=0.6955, LR=0.000041\n",
            "Epoch 3, Loss=1.0412, LR=0.000041\n",
            "Epoch 3, Loss=0.3264, LR=0.000041\n",
            "Epoch 3, Loss=0.9883, LR=0.000041\n",
            "Epoch 3, Loss=0.5097, LR=0.000041\n",
            "Epoch 3, Loss=1.4851, LR=0.000041\n",
            "Epoch 3, Loss=0.6329, LR=0.000041\n",
            "Epoch 3, Loss=0.7400, LR=0.000041\n",
            "Epoch 3, Loss=0.7057, LR=0.000041\n",
            "Epoch 3, Loss=0.9190, LR=0.000040\n",
            "Epoch 3, Loss=0.7460, LR=0.000040\n",
            "Epoch 3, Loss=1.1393, LR=0.000040\n",
            "Epoch 3, Loss=0.6546, LR=0.000040\n",
            "Epoch 3, Loss=0.7539, LR=0.000040\n",
            "Epoch 3, Loss=0.2392, LR=0.000040\n",
            "Epoch 4, Loss=0.2760, LR=0.000040\n",
            "Epoch 4, Loss=0.8489, LR=0.000040\n",
            "Epoch 4, Loss=0.6584, LR=0.000040\n",
            "Epoch 4, Loss=0.6713, LR=0.000040\n",
            "Epoch 4, Loss=0.9246, LR=0.000040\n",
            "Epoch 4, Loss=0.7754, LR=0.000040\n",
            "Epoch 4, Loss=0.5396, LR=0.000039\n",
            "Epoch 4, Loss=0.9189, LR=0.000039\n",
            "Epoch 4, Loss=0.2890, LR=0.000039\n",
            "Epoch 4, Loss=0.4872, LR=0.000039\n",
            "Epoch 4, Loss=0.9036, LR=0.000039\n",
            "Epoch 4, Loss=0.5310, LR=0.000039\n",
            "Epoch 4, Loss=0.5315, LR=0.000039\n",
            "Epoch 4, Loss=0.8786, LR=0.000039\n",
            "Epoch 4, Loss=0.4531, LR=0.000039\n",
            "Epoch 4, Loss=0.5268, LR=0.000039\n",
            "Epoch 4, Loss=0.3318, LR=0.000039\n",
            "Epoch 4, Loss=0.3787, LR=0.000039\n",
            "Epoch 4, Loss=0.6442, LR=0.000038\n",
            "Epoch 4, Loss=0.5544, LR=0.000038\n",
            "Epoch 4, Loss=0.8625, LR=0.000038\n",
            "Epoch 4, Loss=0.2901, LR=0.000038\n",
            "Epoch 4, Loss=0.3383, LR=0.000038\n",
            "Epoch 4, Loss=0.3080, LR=0.000038\n",
            "Epoch 4, Loss=0.3497, LR=0.000038\n",
            "Epoch 4, Loss=0.4635, LR=0.000038\n",
            "Epoch 4, Loss=0.6294, LR=0.000038\n",
            "Epoch 4, Loss=0.4740, LR=0.000038\n",
            "Epoch 4, Loss=0.2296, LR=0.000038\n",
            "Epoch 4, Loss=0.1251, LR=0.000038\n",
            "Epoch 5, Loss=0.0914, LR=0.000037\n",
            "Epoch 5, Loss=0.1728, LR=0.000037\n",
            "Epoch 5, Loss=0.3092, LR=0.000037\n",
            "Epoch 5, Loss=0.1243, LR=0.000037\n",
            "Epoch 5, Loss=0.1997, LR=0.000037\n",
            "Epoch 5, Loss=0.7297, LR=0.000037\n",
            "Epoch 5, Loss=0.1084, LR=0.000037\n",
            "Epoch 5, Loss=0.2923, LR=0.000037\n",
            "Epoch 5, Loss=0.2201, LR=0.000037\n",
            "Epoch 5, Loss=0.4713, LR=0.000037\n",
            "Epoch 5, Loss=0.6035, LR=0.000037\n",
            "Epoch 5, Loss=0.1888, LR=0.000036\n",
            "Epoch 5, Loss=0.1943, LR=0.000036\n",
            "Epoch 5, Loss=0.1379, LR=0.000036\n",
            "Epoch 5, Loss=0.1026, LR=0.000036\n",
            "Epoch 5, Loss=0.3556, LR=0.000036\n",
            "Epoch 5, Loss=0.1662, LR=0.000036\n",
            "Epoch 5, Loss=0.2379, LR=0.000036\n",
            "Epoch 5, Loss=0.5224, LR=0.000036\n",
            "Epoch 5, Loss=0.3386, LR=0.000036\n",
            "Epoch 5, Loss=0.5202, LR=0.000036\n",
            "Epoch 5, Loss=0.0830, LR=0.000036\n",
            "Epoch 5, Loss=0.0722, LR=0.000036\n",
            "Epoch 5, Loss=0.0721, LR=0.000036\n",
            "Epoch 5, Loss=0.3877, LR=0.000035\n",
            "Epoch 5, Loss=0.2762, LR=0.000035\n",
            "Epoch 5, Loss=0.1769, LR=0.000035\n",
            "Epoch 5, Loss=0.2404, LR=0.000035\n",
            "Epoch 5, Loss=0.1703, LR=0.000035\n",
            "Epoch 5, Loss=0.0627, LR=0.000035\n",
            "Epoch 6, Loss=0.0842, LR=0.000035\n",
            "Epoch 6, Loss=0.0650, LR=0.000035\n",
            "Epoch 6, Loss=0.1190, LR=0.000035\n",
            "Epoch 6, Loss=0.0578, LR=0.000035\n",
            "Epoch 6, Loss=0.0928, LR=0.000035\n",
            "Epoch 6, Loss=0.4232, LR=0.000034\n",
            "Epoch 6, Loss=0.0702, LR=0.000034\n",
            "Epoch 6, Loss=0.1460, LR=0.000034\n",
            "Epoch 6, Loss=0.0357, LR=0.000034\n",
            "Epoch 6, Loss=0.0428, LR=0.000034\n",
            "Epoch 6, Loss=0.1058, LR=0.000034\n",
            "Epoch 6, Loss=0.0576, LR=0.000034\n",
            "Epoch 6, Loss=0.1165, LR=0.000034\n",
            "Epoch 6, Loss=0.1106, LR=0.000034\n",
            "Epoch 6, Loss=0.0570, LR=0.000034\n",
            "Epoch 6, Loss=0.0767, LR=0.000034\n",
            "Epoch 6, Loss=0.0673, LR=0.000034\n",
            "Epoch 6, Loss=0.0273, LR=0.000034\n",
            "Epoch 6, Loss=0.4338, LR=0.000033\n",
            "Epoch 6, Loss=0.0536, LR=0.000033\n",
            "Epoch 6, Loss=0.2534, LR=0.000033\n",
            "Epoch 6, Loss=0.2815, LR=0.000033\n",
            "Epoch 6, Loss=0.0568, LR=0.000033\n",
            "Epoch 6, Loss=0.0137, LR=0.000033\n",
            "Epoch 6, Loss=0.0744, LR=0.000033\n",
            "Epoch 6, Loss=0.0345, LR=0.000033\n",
            "Epoch 6, Loss=0.5079, LR=0.000033\n",
            "Epoch 6, Loss=0.0310, LR=0.000033\n",
            "Epoch 6, Loss=0.0445, LR=0.000033\n",
            "Epoch 6, Loss=0.0254, LR=0.000033\n",
            "Epoch 7, Loss=0.0574, LR=0.000032\n",
            "Epoch 7, Loss=0.2385, LR=0.000032\n",
            "Epoch 7, Loss=0.0442, LR=0.000032\n",
            "Epoch 7, Loss=0.1445, LR=0.000032\n",
            "Epoch 7, Loss=0.3782, LR=0.000032\n",
            "Epoch 7, Loss=0.0837, LR=0.000032\n",
            "Epoch 7, Loss=0.0438, LR=0.000032\n",
            "Epoch 7, Loss=0.1691, LR=0.000032\n",
            "Epoch 7, Loss=0.0183, LR=0.000032\n",
            "Epoch 7, Loss=0.0598, LR=0.000032\n",
            "Epoch 7, Loss=0.0901, LR=0.000032\n",
            "Epoch 7, Loss=0.0488, LR=0.000031\n",
            "Epoch 7, Loss=0.0120, LR=0.000031\n",
            "Epoch 7, Loss=0.0241, LR=0.000031\n",
            "Epoch 7, Loss=0.0640, LR=0.000031\n",
            "Epoch 7, Loss=0.0522, LR=0.000031\n",
            "Epoch 7, Loss=0.0171, LR=0.000031\n",
            "Epoch 7, Loss=0.0358, LR=0.000031\n",
            "Epoch 7, Loss=0.0398, LR=0.000031\n",
            "Epoch 7, Loss=0.0395, LR=0.000031\n",
            "Epoch 7, Loss=0.0609, LR=0.000031\n",
            "Epoch 7, Loss=0.0597, LR=0.000031\n",
            "Epoch 7, Loss=0.1079, LR=0.000031\n",
            "Epoch 7, Loss=0.0197, LR=0.000030\n",
            "Epoch 7, Loss=0.0113, LR=0.000030\n",
            "Epoch 7, Loss=0.0500, LR=0.000030\n",
            "Epoch 7, Loss=0.1544, LR=0.000030\n",
            "Epoch 7, Loss=0.1151, LR=0.000030\n",
            "Epoch 7, Loss=0.2707, LR=0.000030\n",
            "Epoch 7, Loss=0.0176, LR=0.000030\n",
            "Epoch 8, Loss=0.0440, LR=0.000030\n",
            "Epoch 8, Loss=0.0314, LR=0.000030\n",
            "Epoch 8, Loss=0.0943, LR=0.000030\n",
            "Epoch 8, Loss=0.0542, LR=0.000030\n",
            "Epoch 8, Loss=0.0620, LR=0.000030\n",
            "Epoch 8, Loss=0.1085, LR=0.000029\n",
            "Epoch 8, Loss=0.0271, LR=0.000029\n",
            "Epoch 8, Loss=0.1118, LR=0.000029\n",
            "Epoch 8, Loss=0.0447, LR=0.000029\n",
            "Epoch 8, Loss=0.0634, LR=0.000029\n",
            "Epoch 8, Loss=0.0534, LR=0.000029\n",
            "Epoch 8, Loss=0.0307, LR=0.000029\n",
            "Epoch 8, Loss=0.0124, LR=0.000029\n",
            "Epoch 8, Loss=0.0254, LR=0.000029\n",
            "Epoch 8, Loss=0.0210, LR=0.000029\n",
            "Epoch 8, Loss=0.0172, LR=0.000029\n",
            "Epoch 8, Loss=0.0570, LR=0.000029\n",
            "Epoch 8, Loss=0.0169, LR=0.000028\n",
            "Epoch 8, Loss=0.1756, LR=0.000028\n",
            "Epoch 8, Loss=0.0288, LR=0.000028\n",
            "Epoch 8, Loss=0.0428, LR=0.000028\n",
            "Epoch 8, Loss=0.0134, LR=0.000028\n",
            "Epoch 8, Loss=0.0064, LR=0.000028\n",
            "Epoch 8, Loss=0.0141, LR=0.000028\n",
            "Epoch 8, Loss=0.0314, LR=0.000028\n",
            "Epoch 8, Loss=0.2121, LR=0.000028\n",
            "Epoch 8, Loss=0.0325, LR=0.000028\n",
            "Epoch 8, Loss=0.0541, LR=0.000028\n",
            "Epoch 8, Loss=0.0405, LR=0.000028\n",
            "Epoch 8, Loss=0.0042, LR=0.000028\n",
            "Epoch 9, Loss=0.0057, LR=0.000027\n",
            "Epoch 9, Loss=0.0047, LR=0.000027\n",
            "Epoch 9, Loss=0.0213, LR=0.000027\n",
            "Epoch 9, Loss=0.0085, LR=0.000027\n",
            "Epoch 9, Loss=0.0230, LR=0.000027\n",
            "Epoch 9, Loss=0.0441, LR=0.000027\n",
            "Epoch 9, Loss=0.0204, LR=0.000027\n",
            "Epoch 9, Loss=0.0195, LR=0.000027\n",
            "Epoch 9, Loss=0.0092, LR=0.000027\n",
            "Epoch 9, Loss=0.0122, LR=0.000027\n",
            "Epoch 9, Loss=0.0218, LR=0.000027\n",
            "Epoch 9, Loss=0.0106, LR=0.000027\n",
            "Epoch 9, Loss=0.0179, LR=0.000026\n",
            "Epoch 9, Loss=0.0217, LR=0.000026\n",
            "Epoch 9, Loss=0.0092, LR=0.000026\n",
            "Epoch 9, Loss=0.0152, LR=0.000026\n",
            "Epoch 9, Loss=0.0119, LR=0.000026\n",
            "Epoch 9, Loss=0.0086, LR=0.000026\n",
            "Epoch 9, Loss=0.0256, LR=0.000026\n",
            "Epoch 9, Loss=0.0173, LR=0.000026\n",
            "Epoch 9, Loss=0.0400, LR=0.000026\n",
            "Epoch 9, Loss=0.0096, LR=0.000026\n",
            "Epoch 9, Loss=0.0110, LR=0.000026\n",
            "Epoch 9, Loss=0.0144, LR=0.000026\n",
            "Epoch 9, Loss=0.0035, LR=0.000025\n",
            "Epoch 9, Loss=0.0066, LR=0.000025\n",
            "Epoch 9, Loss=0.0356, LR=0.000025\n",
            "Epoch 9, Loss=0.0076, LR=0.000025\n",
            "Epoch 9, Loss=0.0212, LR=0.000025\n",
            "Epoch 9, Loss=0.0050, LR=0.000025\n",
            "Epoch 10, Loss=0.0017, LR=0.000025\n",
            "Epoch 10, Loss=0.0032, LR=0.000025\n",
            "Epoch 10, Loss=0.0173, LR=0.000025\n",
            "Epoch 10, Loss=0.0044, LR=0.000025\n",
            "Epoch 10, Loss=0.0170, LR=0.000025\n",
            "Epoch 10, Loss=0.0127, LR=0.000024\n",
            "Epoch 10, Loss=0.0107, LR=0.000024\n",
            "Epoch 10, Loss=0.0132, LR=0.000024\n",
            "Epoch 10, Loss=0.0091, LR=0.000024\n",
            "Epoch 10, Loss=0.0058, LR=0.000024\n",
            "Epoch 10, Loss=0.0136, LR=0.000024\n",
            "Epoch 10, Loss=0.0054, LR=0.000024\n",
            "Epoch 10, Loss=0.0036, LR=0.000024\n",
            "Epoch 10, Loss=0.0100, LR=0.000024\n",
            "Epoch 10, Loss=0.0093, LR=0.000024\n",
            "Epoch 10, Loss=0.0080, LR=0.000024\n",
            "Epoch 10, Loss=0.0080, LR=0.000024\n",
            "Epoch 10, Loss=0.0081, LR=0.000023\n",
            "Epoch 10, Loss=0.0148, LR=0.000023\n",
            "Epoch 10, Loss=0.0181, LR=0.000023\n",
            "Epoch 10, Loss=0.0136, LR=0.000023\n",
            "Epoch 10, Loss=0.0047, LR=0.000023\n",
            "Epoch 10, Loss=0.0161, LR=0.000023\n",
            "Epoch 10, Loss=0.0031, LR=0.000023\n",
            "Epoch 10, Loss=0.0024, LR=0.000023\n",
            "Epoch 10, Loss=0.0041, LR=0.000023\n",
            "Epoch 10, Loss=0.0195, LR=0.000023\n",
            "Epoch 10, Loss=0.0095, LR=0.000023\n",
            "Epoch 10, Loss=0.0162, LR=0.000023\n",
            "Epoch 10, Loss=0.0036, LR=0.000023\n",
            "Epoch 11, Loss=0.0030, LR=0.000022\n",
            "Epoch 11, Loss=0.0021, LR=0.000022\n",
            "Epoch 11, Loss=0.0127, LR=0.000022\n",
            "Epoch 11, Loss=0.0063, LR=0.000022\n",
            "Epoch 11, Loss=0.0128, LR=0.000022\n",
            "Epoch 11, Loss=0.0149, LR=0.000022\n",
            "Epoch 11, Loss=0.0105, LR=0.000022\n",
            "Epoch 11, Loss=0.0090, LR=0.000022\n",
            "Epoch 11, Loss=0.0037, LR=0.000022\n",
            "Epoch 11, Loss=0.0060, LR=0.000022\n",
            "Epoch 11, Loss=0.0112, LR=0.000022\n",
            "Epoch 11, Loss=0.0081, LR=0.000022\n",
            "Epoch 11, Loss=0.0067, LR=0.000021\n",
            "Epoch 11, Loss=0.0088, LR=0.000021\n",
            "Epoch 11, Loss=0.0052, LR=0.000021\n",
            "Epoch 11, Loss=0.0099, LR=0.000021\n",
            "Epoch 11, Loss=0.0054, LR=0.000021\n",
            "Epoch 11, Loss=0.0104, LR=0.000021\n",
            "Epoch 11, Loss=0.0218, LR=0.000021\n",
            "Epoch 11, Loss=0.0098, LR=0.000021\n",
            "Epoch 11, Loss=0.0082, LR=0.000021\n",
            "Epoch 11, Loss=0.0052, LR=0.000021\n",
            "Epoch 11, Loss=0.0042, LR=0.000021\n",
            "Epoch 11, Loss=0.0032, LR=0.000021\n",
            "Epoch 11, Loss=0.0027, LR=0.000020\n",
            "Epoch 11, Loss=0.0039, LR=0.000020\n",
            "Epoch 11, Loss=0.0126, LR=0.000020\n",
            "Epoch 11, Loss=0.0104, LR=0.000020\n",
            "Epoch 11, Loss=0.0063, LR=0.000020\n",
            "Epoch 11, Loss=0.0024, LR=0.000020\n",
            "Epoch 12, Loss=0.0023, LR=0.000020\n",
            "Epoch 12, Loss=0.0028, LR=0.000020\n",
            "Epoch 12, Loss=0.0118, LR=0.000020\n",
            "Epoch 12, Loss=0.0031, LR=0.000020\n",
            "Epoch 12, Loss=0.0051, LR=0.000020\n",
            "Epoch 12, Loss=0.0073, LR=0.000020\n",
            "Epoch 12, Loss=0.0092, LR=0.000019\n",
            "Epoch 12, Loss=0.0087, LR=0.000019\n",
            "Epoch 12, Loss=0.0057, LR=0.000019\n",
            "Epoch 12, Loss=0.0065, LR=0.000019\n",
            "Epoch 12, Loss=0.0067, LR=0.000019\n",
            "Epoch 12, Loss=0.0038, LR=0.000019\n",
            "Epoch 12, Loss=0.0032, LR=0.000019\n",
            "Epoch 12, Loss=0.0079, LR=0.000019\n",
            "Epoch 12, Loss=0.0021, LR=0.000019\n",
            "Epoch 12, Loss=0.0043, LR=0.000019\n",
            "Epoch 12, Loss=0.0056, LR=0.000019\n",
            "Epoch 12, Loss=0.0078, LR=0.000018\n",
            "Epoch 12, Loss=0.0079, LR=0.000018\n",
            "Epoch 12, Loss=0.0057, LR=0.000018\n",
            "Epoch 12, Loss=0.0062, LR=0.000018\n",
            "Epoch 12, Loss=0.0029, LR=0.000018\n",
            "Epoch 12, Loss=0.0045, LR=0.000018\n",
            "Epoch 12, Loss=0.0044, LR=0.000018\n",
            "Epoch 12, Loss=0.0018, LR=0.000018\n",
            "Epoch 12, Loss=0.0017, LR=0.000018\n",
            "Epoch 12, Loss=0.0123, LR=0.000018\n",
            "Epoch 12, Loss=0.0033, LR=0.000018\n",
            "Epoch 12, Loss=0.0058, LR=0.000018\n",
            "Epoch 12, Loss=0.0026, LR=0.000017\n",
            "Epoch 13, Loss=0.0040, LR=0.000017\n",
            "Epoch 13, Loss=0.0020, LR=0.000017\n",
            "Epoch 13, Loss=0.0040, LR=0.000017\n",
            "Epoch 13, Loss=0.0042, LR=0.000017\n",
            "Epoch 13, Loss=0.0158, LR=0.000017\n",
            "Epoch 13, Loss=0.0055, LR=0.000017\n",
            "Epoch 13, Loss=0.0034, LR=0.000017\n",
            "Epoch 13, Loss=0.0058, LR=0.000017\n",
            "Epoch 13, Loss=0.0028, LR=0.000017\n",
            "Epoch 13, Loss=0.0041, LR=0.000017\n",
            "Epoch 13, Loss=0.0043, LR=0.000017\n",
            "Epoch 13, Loss=0.0031, LR=0.000017\n",
            "Epoch 13, Loss=0.0035, LR=0.000016\n",
            "Epoch 13, Loss=0.0035, LR=0.000016\n",
            "Epoch 13, Loss=0.0041, LR=0.000016\n",
            "Epoch 13, Loss=0.0050, LR=0.000016\n",
            "Epoch 13, Loss=0.0047, LR=0.000016\n",
            "Epoch 13, Loss=0.0053, LR=0.000016\n",
            "Epoch 13, Loss=0.0100, LR=0.000016\n",
            "Epoch 13, Loss=0.0073, LR=0.000016\n",
            "Epoch 13, Loss=0.0073, LR=0.000016\n",
            "Epoch 13, Loss=0.0029, LR=0.000016\n",
            "Epoch 13, Loss=0.0038, LR=0.000016\n",
            "Epoch 13, Loss=0.0019, LR=0.000016\n",
            "Epoch 13, Loss=0.0018, LR=0.000015\n",
            "Epoch 13, Loss=0.0030, LR=0.000015\n",
            "Epoch 13, Loss=0.0085, LR=0.000015\n",
            "Epoch 13, Loss=0.0027, LR=0.000015\n",
            "Epoch 13, Loss=0.0069, LR=0.000015\n",
            "Epoch 13, Loss=0.0018, LR=0.000015\n",
            "Epoch 14, Loss=0.0019, LR=0.000015\n",
            "Epoch 14, Loss=0.0020, LR=0.000015\n",
            "Epoch 14, Loss=0.0051, LR=0.000015\n",
            "Epoch 14, Loss=0.0029, LR=0.000015\n",
            "Epoch 14, Loss=0.0070, LR=0.000015\n",
            "Epoch 14, Loss=0.0032, LR=0.000015\n",
            "Epoch 14, Loss=0.0032, LR=0.000014\n",
            "Epoch 14, Loss=0.0060, LR=0.000014\n",
            "Epoch 14, Loss=0.0037, LR=0.000014\n",
            "Epoch 14, Loss=0.0052, LR=0.000014\n",
            "Epoch 14, Loss=0.0040, LR=0.000014\n",
            "Epoch 14, Loss=0.0035, LR=0.000014\n",
            "Epoch 14, Loss=0.0038, LR=0.000014\n",
            "Epoch 14, Loss=0.0046, LR=0.000014\n",
            "Epoch 14, Loss=0.0025, LR=0.000014\n",
            "Epoch 14, Loss=0.0054, LR=0.000014\n",
            "Epoch 14, Loss=0.0063, LR=0.000014\n",
            "Epoch 14, Loss=0.0048, LR=0.000014\n",
            "Epoch 14, Loss=0.0052, LR=0.000013\n",
            "Epoch 14, Loss=0.0057, LR=0.000013\n",
            "Epoch 14, Loss=0.0216, LR=0.000013\n",
            "Epoch 14, Loss=0.0027, LR=0.000013\n",
            "Epoch 14, Loss=0.0025, LR=0.000013\n",
            "Epoch 14, Loss=0.0034, LR=0.000013\n",
            "Epoch 14, Loss=0.0017, LR=0.000013\n",
            "Epoch 14, Loss=0.0026, LR=0.000013\n",
            "Epoch 14, Loss=0.0088, LR=0.000013\n",
            "Epoch 14, Loss=0.0053, LR=0.000013\n",
            "Epoch 14, Loss=0.0052, LR=0.000013\n",
            "Epoch 14, Loss=0.0029, LR=0.000013\n",
            "Epoch 15, Loss=0.0015, LR=0.000012\n",
            "Epoch 15, Loss=0.0023, LR=0.000012\n",
            "Epoch 15, Loss=0.0047, LR=0.000012\n",
            "Epoch 15, Loss=0.0045, LR=0.000012\n",
            "Epoch 15, Loss=0.0070, LR=0.000012\n",
            "Epoch 15, Loss=0.0048, LR=0.000012\n",
            "Epoch 15, Loss=0.0041, LR=0.000012\n",
            "Epoch 15, Loss=0.0054, LR=0.000012\n",
            "Epoch 15, Loss=0.0036, LR=0.000012\n",
            "Epoch 15, Loss=0.0046, LR=0.000012\n",
            "Epoch 15, Loss=0.0031, LR=0.000012\n",
            "Epoch 15, Loss=0.0044, LR=0.000012\n",
            "Epoch 15, Loss=0.0037, LR=0.000011\n",
            "Epoch 15, Loss=0.0056, LR=0.000011\n",
            "Epoch 15, Loss=0.0027, LR=0.000011\n",
            "Epoch 15, Loss=0.0035, LR=0.000011\n",
            "Epoch 15, Loss=0.0051, LR=0.000011\n",
            "Epoch 15, Loss=0.0039, LR=0.000011\n",
            "Epoch 15, Loss=0.0064, LR=0.000011\n",
            "Epoch 15, Loss=0.0081, LR=0.000011\n",
            "Epoch 15, Loss=0.0047, LR=0.000011\n",
            "Epoch 15, Loss=0.0033, LR=0.000011\n",
            "Epoch 15, Loss=0.0044, LR=0.000011\n",
            "Epoch 15, Loss=0.0027, LR=0.000010\n",
            "Epoch 15, Loss=0.0024, LR=0.000010\n",
            "Epoch 15, Loss=0.0028, LR=0.000010\n",
            "Epoch 15, Loss=0.0059, LR=0.000010\n",
            "Epoch 15, Loss=0.0065, LR=0.000010\n",
            "Epoch 15, Loss=0.0069, LR=0.000010\n",
            "Epoch 15, Loss=0.0019, LR=0.000010\n",
            "Epoch 16, Loss=0.0017, LR=0.000010\n",
            "Epoch 16, Loss=0.0018, LR=0.000010\n",
            "Epoch 16, Loss=0.0059, LR=0.000010\n",
            "Epoch 16, Loss=0.0023, LR=0.000010\n",
            "Epoch 16, Loss=0.0049, LR=0.000010\n",
            "Epoch 16, Loss=0.0069, LR=0.000010\n",
            "Epoch 16, Loss=0.0023, LR=0.000009\n",
            "Epoch 16, Loss=0.0077, LR=0.000009\n",
            "Epoch 16, Loss=0.0026, LR=0.000009\n",
            "Epoch 16, Loss=0.0044, LR=0.000009\n",
            "Epoch 16, Loss=0.0038, LR=0.000009\n",
            "Epoch 16, Loss=0.0068, LR=0.000009\n",
            "Epoch 16, Loss=0.0035, LR=0.000009\n",
            "Epoch 16, Loss=0.0035, LR=0.000009\n",
            "Epoch 16, Loss=0.0028, LR=0.000009\n",
            "Epoch 16, Loss=0.0028, LR=0.000009\n",
            "Epoch 16, Loss=0.0024, LR=0.000009\n",
            "Epoch 16, Loss=0.0031, LR=0.000009\n",
            "Epoch 16, Loss=0.0038, LR=0.000008\n",
            "Epoch 16, Loss=0.0053, LR=0.000008\n",
            "Epoch 16, Loss=0.0044, LR=0.000008\n",
            "Epoch 16, Loss=0.0060, LR=0.000008\n",
            "Epoch 16, Loss=0.0025, LR=0.000008\n",
            "Epoch 16, Loss=0.0023, LR=0.000008\n",
            "Epoch 16, Loss=0.0016, LR=0.000008\n",
            "Epoch 16, Loss=0.0024, LR=0.000008\n",
            "Epoch 16, Loss=0.0080, LR=0.000008\n",
            "Epoch 16, Loss=0.0044, LR=0.000008\n",
            "Epoch 16, Loss=0.0049, LR=0.000008\n",
            "Epoch 16, Loss=0.0023, LR=0.000008\n",
            "Epoch 17, Loss=0.0011, LR=0.000007\n",
            "Epoch 17, Loss=0.0018, LR=0.000007\n",
            "Epoch 17, Loss=0.0025, LR=0.000007\n",
            "Epoch 17, Loss=0.0025, LR=0.000007\n",
            "Epoch 17, Loss=0.0045, LR=0.000007\n",
            "Epoch 17, Loss=0.0034, LR=0.000007\n",
            "Epoch 17, Loss=0.0031, LR=0.000007\n",
            "Epoch 17, Loss=0.0039, LR=0.000007\n",
            "Epoch 17, Loss=0.0026, LR=0.000007\n",
            "Epoch 17, Loss=0.0020, LR=0.000007\n",
            "Epoch 17, Loss=0.0036, LR=0.000007\n",
            "Epoch 17, Loss=0.0034, LR=0.000007\n",
            "Epoch 17, Loss=0.0029, LR=0.000006\n",
            "Epoch 17, Loss=0.0027, LR=0.000006\n",
            "Epoch 17, Loss=0.0019, LR=0.000006\n",
            "Epoch 17, Loss=0.0042, LR=0.000006\n",
            "Epoch 17, Loss=0.0027, LR=0.000006\n",
            "Epoch 17, Loss=0.0052, LR=0.000006\n",
            "Epoch 17, Loss=0.0069, LR=0.000006\n",
            "Epoch 17, Loss=0.0062, LR=0.000006\n",
            "Epoch 17, Loss=0.0040, LR=0.000006\n",
            "Epoch 17, Loss=0.0025, LR=0.000006\n",
            "Epoch 17, Loss=0.0022, LR=0.000006\n",
            "Epoch 17, Loss=0.0021, LR=0.000006\n",
            "Epoch 17, Loss=0.0013, LR=0.000005\n",
            "Epoch 17, Loss=0.0023, LR=0.000005\n",
            "Epoch 17, Loss=0.0097, LR=0.000005\n",
            "Epoch 17, Loss=0.0028, LR=0.000005\n",
            "Epoch 17, Loss=0.0072, LR=0.000005\n",
            "Epoch 17, Loss=0.0026, LR=0.000005\n",
            "Epoch 18, Loss=0.0022, LR=0.000005\n",
            "Epoch 18, Loss=0.0012, LR=0.000005\n",
            "Epoch 18, Loss=0.0021, LR=0.000005\n",
            "Epoch 18, Loss=0.0023, LR=0.000005\n",
            "Epoch 18, Loss=0.0046, LR=0.000005\n",
            "Epoch 18, Loss=0.0034, LR=0.000005\n",
            "Epoch 18, Loss=0.0042, LR=0.000004\n",
            "Epoch 18, Loss=0.0052, LR=0.000004\n",
            "Epoch 18, Loss=0.0033, LR=0.000004\n",
            "Epoch 18, Loss=0.0063, LR=0.000004\n",
            "Epoch 18, Loss=0.0055, LR=0.000004\n",
            "Epoch 18, Loss=0.0040, LR=0.000004\n",
            "Epoch 18, Loss=0.0029, LR=0.000004\n",
            "Epoch 18, Loss=0.0036, LR=0.000004\n",
            "Epoch 18, Loss=0.0071, LR=0.000004\n",
            "Epoch 18, Loss=0.0069, LR=0.000004\n",
            "Epoch 18, Loss=0.0033, LR=0.000004\n",
            "Epoch 18, Loss=0.0033, LR=0.000004\n",
            "Epoch 18, Loss=0.0063, LR=0.000003\n",
            "Epoch 18, Loss=0.0032, LR=0.000003\n",
            "Epoch 18, Loss=0.0064, LR=0.000003\n",
            "Epoch 18, Loss=0.0022, LR=0.000003\n",
            "Epoch 18, Loss=0.0026, LR=0.000003\n",
            "Epoch 18, Loss=0.0023, LR=0.000003\n",
            "Epoch 18, Loss=0.0018, LR=0.000003\n",
            "Epoch 18, Loss=0.0018, LR=0.000003\n",
            "Epoch 18, Loss=0.0080, LR=0.000003\n",
            "Epoch 18, Loss=0.0017, LR=0.000003\n",
            "Epoch 18, Loss=0.0032, LR=0.000003\n",
            "Epoch 18, Loss=0.0010, LR=0.000003\n",
            "Epoch 19, Loss=0.0011, LR=0.000002\n",
            "Epoch 19, Loss=0.0028, LR=0.000002\n",
            "Epoch 19, Loss=0.0062, LR=0.000002\n",
            "Epoch 19, Loss=0.0020, LR=0.000002\n",
            "Epoch 19, Loss=0.0053, LR=0.000002\n",
            "Epoch 19, Loss=0.0046, LR=0.000002\n",
            "Epoch 19, Loss=0.0030, LR=0.000002\n",
            "Epoch 19, Loss=0.0052, LR=0.000002\n",
            "Epoch 19, Loss=0.0030, LR=0.000002\n",
            "Epoch 19, Loss=0.0038, LR=0.000002\n",
            "Epoch 19, Loss=0.0035, LR=0.000002\n",
            "Epoch 19, Loss=0.0032, LR=0.000002\n",
            "Epoch 19, Loss=0.0034, LR=0.000001\n",
            "Epoch 19, Loss=0.0049, LR=0.000001\n",
            "Epoch 19, Loss=0.0036, LR=0.000001\n",
            "Epoch 19, Loss=0.0057, LR=0.000001\n",
            "Epoch 19, Loss=0.0033, LR=0.000001\n",
            "Epoch 19, Loss=0.0052, LR=0.000001\n",
            "Epoch 19, Loss=0.0046, LR=0.000001\n",
            "Epoch 19, Loss=0.0065, LR=0.000001\n",
            "Epoch 19, Loss=0.0052, LR=0.000001\n",
            "Epoch 19, Loss=0.0044, LR=0.000001\n",
            "Epoch 19, Loss=0.0026, LR=0.000001\n",
            "Epoch 19, Loss=0.0022, LR=0.000001\n",
            "Epoch 19, Loss=0.0016, LR=0.000000\n",
            "Epoch 19, Loss=0.0017, LR=0.000000\n",
            "Epoch 19, Loss=0.0070, LR=0.000000\n",
            "Epoch 19, Loss=0.0036, LR=0.000000\n",
            "Epoch 19, Loss=0.0038, LR=0.000000\n",
            "Epoch 19, Loss=0.0019, LR=0.000000\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.78      0.72        23\n",
            "           1       0.87      0.84      0.86        57\n",
            "           2       0.58      0.52      0.55        21\n",
            "\n",
            "    accuracy                           0.76       101\n",
            "   macro avg       0.71      0.72      0.71       101\n",
            "weighted avg       0.76      0.76      0.76       101\n",
            "\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW, get_scheduler\n",
        "import sklearn.model_selection as model_selection\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "# Map string labels to integer labels\n",
        "label_map = {'Negativa': 0, 'Positiva': 1, 'Neutra': 2}\n",
        "ytrain_global = np.array([label_map[label] for label in df_treino['choices']])\n",
        "xtrain_global = np.array(df_treino['noticia'])\n",
        "\n",
        "\n",
        "\n",
        "xtrain, xval, ytrain, yval = model_selection.train_test_split(xtrain_global, ytrain_global, test_size=0.30, random_state=42,shuffle=True)\n",
        "\n",
        "train_encodings = tokenizer(xtrain.tolist(), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
        "val_encodings = tokenizer(xval.tolist(), truncation=True, padding=True,max_length=512, return_tensors='pt')\n",
        "\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        label = torch.tensor(self.labels[idx])\n",
        "        return (item,label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = MyDataset(train_encodings, ytrain)\n",
        "val_dataset = MyDataset(val_encodings, yval)\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "model = LIABertClassifier(modelo_soja, 3)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "dl_train = DataLoader(train_dataset,batch_size=8)\n",
        "dl_eval  = DataLoader(val_dataset,batch_size=8)\n",
        "x,y = next(iter(dl_train))\n",
        "\n",
        "batch = {k: v.to(device) for k, v in x.items()}\n",
        "model.to(device)\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=0.000050)\n",
        "\n",
        "loss_fct = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "from transformers import get_scheduler\n",
        "\n",
        "scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optim,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_epochs*len(dl_train)\n",
        ")\n",
        "\n",
        "# scheduler = OneCycleLR(optim, max_lr=max_lr, total_steps=total_steps,\n",
        "#                        div_factor=div_factor, pct_start=pct_start)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    lepochs = []\n",
        "    for batch,y in dl_train:\n",
        "        model.train()\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        y = y.to(device)\n",
        "        outputs = model(**batch)\n",
        "        loss = loss_fct(outputs,y)\n",
        "        lepochs.append(loss.cpu().item())\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        scheduler.step()\n",
        "        optim.zero_grad()\n",
        "        current_lr = optim.param_groups[0]['lr']\n",
        "\n",
        "        print(f\"Epoch {epoch}, Loss={loss:.4f}, LR={current_lr:.6f}\")\n",
        "\n",
        "model.eval()\n",
        "ytrue = []\n",
        "ypred = []\n",
        "for batch,y in dl_eval:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    y = y.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "        ytrue.extend(y.cpu().numpy().tolist())\n",
        "        ypred.extend(outputs.argmax(axis=1).cpu().numpy().tolist())\n",
        "\n",
        "print(classification_report(ytrue,ypred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), '/content/drive/MyDrive/modelo_soja.pt')\n",
        "\n",
        "## predict from string\n",
        "frase = \"\"\"\n",
        "O preço da soja em Paranaguá (PR) em abril registrou o maior aumento médio mensal desde agosto de 2020,\n",
        "informa o Centro de Estudos Avançados em Economia Aplicada (Cepea), da Esalq/USP.\n",
        "Considerando a inflação, a média de R$ 145,24 por saca de 60 quilos seria a melhor desde março de 2020.\n",
        "\"\"\"\n",
        "frase = preprocess_text(frase)\n",
        "frase = tokenizer(frase, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
        "frase = {k: v.to(device) for k, v in frase.items()}\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**frase)\n",
        "    if outputs.argmax(axis=1).cpu().numpy().tolist()[0] == 0:\n",
        "        print('Negativa')\n",
        "    elif outputs.argmax(axis=1).cpu().numpy().tolist()[0] == 1:\n",
        "        print('Positiva')\n",
        "    else:\n",
        "        print('Neutra')\n",
        "    print(outputs.argmax(axis=1).cpu().numpy().tolist())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKayyOlzz_nt",
        "outputId": "be203876-b7c7-46d8-8860-5e331f6b21ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positiva\n",
            "[1]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bdgd",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}