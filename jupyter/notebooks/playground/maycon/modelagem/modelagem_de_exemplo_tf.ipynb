{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file_upload                 294\n",
       "drafts                      294\n",
       "predictions                 294\n",
       "meta                        294\n",
       "created_at                  294\n",
       "updated_at                  294\n",
       "inner_id                    294\n",
       "total_annotations           294\n",
       "cancelled_annotations       294\n",
       "total_predictions           294\n",
       "comment_count               294\n",
       "unresolved_comment_count    294\n",
       "last_comment_updated_at       0\n",
       "project                     294\n",
       "updated_by                  294\n",
       "comment_authors             294\n",
       "completed_by                294\n",
       "was_cancelled               294\n",
       "ground_truth                294\n",
       "created_at                  294\n",
       "updated_at                  294\n",
       "lead_time                   294\n",
       "prediction                  294\n",
       "result_count                294\n",
       "unique_id                   294\n",
       "last_action                   0\n",
       "task                        294\n",
       "project                     294\n",
       "updated_by                  294\n",
       "parent_prediction             0\n",
       "parent_annotation             0\n",
       "last_created_by               0\n",
       "id                          294\n",
       "type                        294\n",
       "origin                      294\n",
       "to_name                     294\n",
       "from_name                   294\n",
       "0                             0\n",
       "choices                     294\n",
       "0                             0\n",
       "ano                         294\n",
       "mes                         294\n",
       "url                         294\n",
       "data                        294\n",
       "titulo                      294\n",
       "noticia                     294\n",
       "data_hora                   294\n",
       "Unnamed: 0                  294\n",
       "qtde_palavras_soja          294\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from unidecode import unidecode\n",
    "## ler arquivo json em pandas\n",
    "df = pd.read_json('/root/projects/personal/comitiva_esperanca/label-studio/data/export/project-1-at-2023-05-07-03-57-20fa509e.json')\n",
    "\n",
    "df = df.drop(['id'], axis=1)\n",
    "## expandir coluna annotations\n",
    "df = pd.concat([df.drop(['annotations'], axis=1), df['annotations'].apply(pd.Series)], axis=1)\n",
    "## expandir coluna 0 e renomear para annotations\n",
    "df = pd.concat([df.drop([0], axis=1), df[0].apply(pd.Series)], axis=1)\n",
    "## expandir coluna result e renomear para result\n",
    "df = pd.concat([df.drop(['result'], axis=1), df['result'].apply(pd.Series)], axis=1)\n",
    "df = df.drop(['id'], axis=1)\n",
    "\n",
    "## expandir coluna 0 e renomear para result\n",
    "df = pd.concat([df.drop([0], axis=1), df[0].apply(pd.Series)], axis=1)\n",
    "## expandir coluna value e renomear para value\n",
    "df = pd.concat([df.drop(['value'], axis=1), df['value'].apply(pd.Series)], axis=1)\n",
    "## dropar choices nulos\n",
    "df = df.dropna(subset=['choices'])\n",
    "## obter choices \n",
    "df['choices'] = df['choices'].apply(lambda x: x[0])\n",
    "\n",
    "## expandir coluna data\n",
    "df = pd.concat([df.drop(['data'], axis=1), df['data'].apply(pd.Series)], axis=1)\n",
    "\n",
    "df_noticia_original = df.copy()\n",
    "\n",
    "\n",
    "padrao_data_cepea = r\"Cepea, \\d{2}/\\d{2}/\\d{4} - \"\n",
    "df['noticia'] = df['noticia'].apply(lambda x: re.sub(padrao_data_cepea, '', x))\n",
    "\n",
    "## remover a palavra 'cepea' das noticias\n",
    "padrao_cepea = r\"Cepea\"\n",
    "df['noticia'] = df['noticia'].apply(lambda x: re.sub(padrao_cepea, '', x, flags=re.IGNORECASE))\n",
    "\n",
    "## remover numeros das noticias\n",
    "padrao_numeros = r'[0-9]+'\n",
    "df['noticia'] = df['noticia'].apply(lambda x: re.sub(padrao_numeros, '', x))\n",
    "\n",
    "## noticia que contem a palavra 'soja'\n",
    "df = df[df['titulo'].str.contains('soja', flags=re.IGNORECASE)]\n",
    "\n",
    "## remover noticias com choice 'desclassificar'\n",
    "df = df[df['choices'] != 'Desclassificar']\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 01:05:54.946062: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-07 01:05:55.407216: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-07 01:05:55.408928: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-07 01:05:57.057716: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/root/projects/personal/comitiva_esperanca/.python_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-05-07 01:06:00.319066: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-07 01:06:00.319634: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 1s 46ms/step - loss: 0.6932 - accuracy: 0.4000 - val_loss: 0.6926 - val_accuracy: 0.4270\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6907 - accuracy: 0.4780 - val_loss: 0.6922 - val_accuracy: 0.4270\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6884 - accuracy: 0.4780 - val_loss: 0.6916 - val_accuracy: 0.4270\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6855 - accuracy: 0.4780 - val_loss: 0.6912 - val_accuracy: 0.4270\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6817 - accuracy: 0.4780 - val_loss: 0.6902 - val_accuracy: 0.4270\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6776 - accuracy: 0.4780 - val_loss: 0.6890 - val_accuracy: 0.4270\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6724 - accuracy: 0.4780 - val_loss: 0.6867 - val_accuracy: 0.4270\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6650 - accuracy: 0.4780 - val_loss: 0.6844 - val_accuracy: 0.4270\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6547 - accuracy: 0.4780 - val_loss: 0.6795 - val_accuracy: 0.4270\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6398 - accuracy: 0.5171 - val_loss: 0.6700 - val_accuracy: 0.4270\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6193 - accuracy: 0.5707 - val_loss: 0.6581 - val_accuracy: 0.4382\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.5895 - accuracy: 0.6488 - val_loss: 0.6415 - val_accuracy: 0.5056\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.5520 - accuracy: 0.6927 - val_loss: 0.6175 - val_accuracy: 0.6180\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.5057 - accuracy: 0.7463 - val_loss: 0.5921 - val_accuracy: 0.6517\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.4540 - accuracy: 0.7805 - val_loss: 0.5605 - val_accuracy: 0.6629\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3988 - accuracy: 0.7854 - val_loss: 0.5312 - val_accuracy: 0.6629\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3469 - accuracy: 0.8000 - val_loss: 0.5033 - val_accuracy: 0.6629\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2975 - accuracy: 0.8000 - val_loss: 0.4838 - val_accuracy: 0.6629\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2605 - accuracy: 0.8000 - val_loss: 0.4700 - val_accuracy: 0.6517\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.2321 - accuracy: 0.8000 - val_loss: 0.4634 - val_accuracy: 0.6629\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2102 - accuracy: 0.8049 - val_loss: 0.4632 - val_accuracy: 0.6629\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1944 - accuracy: 0.8049 - val_loss: 0.4676 - val_accuracy: 0.6629\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.1821 - accuracy: 0.8049 - val_loss: 0.4702 - val_accuracy: 0.6629\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.1718 - accuracy: 0.8049 - val_loss: 0.4742 - val_accuracy: 0.6742\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.1639 - accuracy: 0.8049 - val_loss: 0.4792 - val_accuracy: 0.6742\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1579 - accuracy: 0.8049 - val_loss: 0.4844 - val_accuracy: 0.6629\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1533 - accuracy: 0.8049 - val_loss: 0.4904 - val_accuracy: 0.6629\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1502 - accuracy: 0.8049 - val_loss: 0.4973 - val_accuracy: 0.6629\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1478 - accuracy: 0.8049 - val_loss: 0.5052 - val_accuracy: 0.6629\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1456 - accuracy: 0.8049 - val_loss: 0.5115 - val_accuracy: 0.6629\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1441 - accuracy: 0.8049 - val_loss: 0.5188 - val_accuracy: 0.6629\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1433 - accuracy: 0.8049 - val_loss: 0.5238 - val_accuracy: 0.6629\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1421 - accuracy: 0.8049 - val_loss: 0.5302 - val_accuracy: 0.6629\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1414 - accuracy: 0.8049 - val_loss: 0.5349 - val_accuracy: 0.6629\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1416 - accuracy: 0.8049 - val_loss: 0.5401 - val_accuracy: 0.6629\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1407 - accuracy: 0.8049 - val_loss: 0.5468 - val_accuracy: 0.6629\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1404 - accuracy: 0.8049 - val_loss: 0.5497 - val_accuracy: 0.6629\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1401 - accuracy: 0.8049 - val_loss: 0.5522 - val_accuracy: 0.6629\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1393 - accuracy: 0.8049 - val_loss: 0.5573 - val_accuracy: 0.6629\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1392 - accuracy: 0.8049 - val_loss: 0.5613 - val_accuracy: 0.6629\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1388 - accuracy: 0.8049 - val_loss: 0.5652 - val_accuracy: 0.6629\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1383 - accuracy: 0.8049 - val_loss: 0.5681 - val_accuracy: 0.6629\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1382 - accuracy: 0.8049 - val_loss: 0.5714 - val_accuracy: 0.6629\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1379 - accuracy: 0.8049 - val_loss: 0.5761 - val_accuracy: 0.6629\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1379 - accuracy: 0.8049 - val_loss: 0.5814 - val_accuracy: 0.6629\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1378 - accuracy: 0.8049 - val_loss: 0.5833 - val_accuracy: 0.6629\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.1376 - accuracy: 0.8049 - val_loss: 0.5864 - val_accuracy: 0.6629\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1375 - accuracy: 0.8049 - val_loss: 0.5888 - val_accuracy: 0.6629\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.1376 - accuracy: 0.8049 - val_loss: 0.5936 - val_accuracy: 0.6629\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1375 - accuracy: 0.8049 - val_loss: 0.5960 - val_accuracy: 0.6629\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.1371 - accuracy: 0.8049 - val_loss: 0.6002 - val_accuracy: 0.6629\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.1377 - accuracy: 0.8049 - val_loss: 0.6022 - val_accuracy: 0.6629\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.1379 - accuracy: 0.8049 - val_loss: 0.6074 - val_accuracy: 0.6629\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.1388 - accuracy: 0.8049 - val_loss: 0.6085 - val_accuracy: 0.6742\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1391 - accuracy: 0.8049 - val_loss: 0.6088 - val_accuracy: 0.6629\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1384 - accuracy: 0.8049 - val_loss: 0.6116 - val_accuracy: 0.6629\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.1376 - accuracy: 0.8049 - val_loss: 0.6141 - val_accuracy: 0.6629\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.1401 - accuracy: 0.8049 - val_loss: 0.6179 - val_accuracy: 0.6742\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.1399 - accuracy: 0.8049 - val_loss: 0.6219 - val_accuracy: 0.6629\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.1474 - accuracy: 0.8049 - val_loss: 0.6311 - val_accuracy: 0.6854\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.1476 - accuracy: 0.8049 - val_loss: 0.6059 - val_accuracy: 0.6629\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.1448 - accuracy: 0.8049 - val_loss: 0.6117 - val_accuracy: 0.6742\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1467 - accuracy: 0.8049 - val_loss: 0.6167 - val_accuracy: 0.6742\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.1445 - accuracy: 0.8049 - val_loss: 0.6080 - val_accuracy: 0.6629\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.1458 - accuracy: 0.8049 - val_loss: 0.6236 - val_accuracy: 0.6629\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1489 - accuracy: 0.8049 - val_loss: 0.6158 - val_accuracy: 0.6742\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1433 - accuracy: 0.8049 - val_loss: 0.5986 - val_accuracy: 0.6629\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1446 - accuracy: 0.8049 - val_loss: 0.6058 - val_accuracy: 0.6742\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1449 - accuracy: 0.8049 - val_loss: 0.6129 - val_accuracy: 0.6742\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.1439 - accuracy: 0.8049 - val_loss: 0.6013 - val_accuracy: 0.6629\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1430 - accuracy: 0.8049 - val_loss: 0.5962 - val_accuracy: 0.6629\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1405 - accuracy: 0.8049 - val_loss: 0.6019 - val_accuracy: 0.6742\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1412 - accuracy: 0.8049 - val_loss: 0.6017 - val_accuracy: 0.6742\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1399 - accuracy: 0.8049 - val_loss: 0.6016 - val_accuracy: 0.6742\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1403 - accuracy: 0.8049 - val_loss: 0.5931 - val_accuracy: 0.6742\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1396 - accuracy: 0.8049 - val_loss: 0.5948 - val_accuracy: 0.6629\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.1384 - accuracy: 0.8049 - val_loss: 0.5991 - val_accuracy: 0.6629\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.1374 - accuracy: 0.8049 - val_loss: 0.6032 - val_accuracy: 0.6629\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.1369 - accuracy: 0.8049 - val_loss: 0.6045 - val_accuracy: 0.6629\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1370 - accuracy: 0.8049 - val_loss: 0.6073 - val_accuracy: 0.6629\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1370 - accuracy: 0.8049 - val_loss: 0.6071 - val_accuracy: 0.6629\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1367 - accuracy: 0.8049 - val_loss: 0.6086 - val_accuracy: 0.6629\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1367 - accuracy: 0.8049 - val_loss: 0.6100 - val_accuracy: 0.6629\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.1366 - accuracy: 0.8049 - val_loss: 0.6123 - val_accuracy: 0.6629\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.1365 - accuracy: 0.8049 - val_loss: 0.6151 - val_accuracy: 0.6629\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1365 - accuracy: 0.8049 - val_loss: 0.6166 - val_accuracy: 0.6629\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1365 - accuracy: 0.8049 - val_loss: 0.6185 - val_accuracy: 0.6629\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1364 - accuracy: 0.8049 - val_loss: 0.6207 - val_accuracy: 0.6629\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1365 - accuracy: 0.8049 - val_loss: 0.6223 - val_accuracy: 0.6629\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.1364 - accuracy: 0.8049 - val_loss: 0.6243 - val_accuracy: 0.6629\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1363 - accuracy: 0.8049 - val_loss: 0.6263 - val_accuracy: 0.6629\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1362 - accuracy: 0.8049 - val_loss: 0.6282 - val_accuracy: 0.6629\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.1362 - accuracy: 0.8049 - val_loss: 0.6302 - val_accuracy: 0.6629\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.1361 - accuracy: 0.8049 - val_loss: 0.6307 - val_accuracy: 0.6629\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.1361 - accuracy: 0.8049 - val_loss: 0.6319 - val_accuracy: 0.6629\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.1360 - accuracy: 0.8049 - val_loss: 0.6343 - val_accuracy: 0.6629\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1360 - accuracy: 0.8049 - val_loss: 0.6365 - val_accuracy: 0.6629\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.1360 - accuracy: 0.8049 - val_loss: 0.6383 - val_accuracy: 0.6629\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.1360 - accuracy: 0.8049 - val_loss: 0.6397 - val_accuracy: 0.6629\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1360 - accuracy: 0.8049 - val_loss: 0.6413 - val_accuracy: 0.6629\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6363 - accuracy: 0.6786\n",
      "Test accuracy: 0.6785714030265808\n"
     ]
    }
   ],
   "source": [
    "# Selecionar apenas as colunas necessárias\n",
    "columns_to_select = ['id', 'data', 'noticia', 'titulo', 'choices', 'unique_id']\n",
    "\n",
    "df = df[columns_to_select]\n",
    "df.dropna(subset=['noticia'])\n",
    "\n",
    "# Pré-processamento dos dados\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # remover acentuação\n",
    "    text = unidecode(text)\n",
    "    # Remover pontuações\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenização\n",
    "    words = word_tokenize(text.lower())\n",
    "    # Remover stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['noticia'] = df['noticia'].apply(preprocess_text)\n",
    "\n",
    "df_treino = df[:210]\n",
    "df_validacao = df[210:]\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Criar modelo de classificação de sentimento\n",
    "vocab_size = 1000\n",
    "embedding_dim = 100\n",
    "max_length = 125\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dense(50, activation='relu'),\n",
    "    keras.layers.Dense(25, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Pré-processar os dados\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df['noticia'])\n",
    "sequences = tokenizer.texts_to_sequences(df['noticia'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "labels = df['choices'].map({'Positiva': 1, 'Negativa': 0, 'Neutra': 0.5})\n",
    "\n",
    "# Treinar o modelo\n",
    "history = model.fit(padded_sequences, labels, epochs=100, validation_split=0.3)\n",
    "\n",
    "# Avaliar o modelo\n",
    "test_sequences = tokenizer.texts_to_sequences(df_validacao['noticia'])\n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "test_labels = df_validacao['choices'].map({'Positiva': 1, 'Negativa': 0, 'Neutra': 0.5})\n",
    "test_loss, test_accuracy = model.evaluate(padded_test_sequences, test_labels)\n",
    "print('Test accuracy:', test_accuracy)\n",
    "\n",
    "# Salvar o modelo\n",
    "model.save('modelo.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>data</th>\n",
       "      <th>noticia</th>\n",
       "      <th>titulo</th>\n",
       "      <th>choices</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>predicao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>9jPu-wLJ6L</td>\n",
       "      <td>27/05/2019</td>\n",
       "      <td>maiores demandas interna externa farelo oleo s...</td>\n",
       "      <td>SOJA/CEPEA: Demanda aquecida por farelo e óleo...</td>\n",
       "      <td>Positiva</td>\n",
       "      <td>701c2b37-164f-4dc7-980c-e81234625d92</td>\n",
       "      <td>Positiva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>ASTg8bJjN5</td>\n",
       "      <td></td>\n",
       "      <td>boa parte safra soja brasileira ja comercializ...</td>\n",
       "      <td>SOJA/CEPEA: Com menor ritmo de embarques, prêm...</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>85ba023a-e6c6-4768-967a-58d52de40d92</td>\n",
       "      <td>Negativa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>NVXB7cSsqU</td>\n",
       "      <td>17/10/2022</td>\n",
       "      <td>precos soja voltaram subir mercado interno sem...</td>\n",
       "      <td>SOJA/CEPEA: Valores reagem no Brasil</td>\n",
       "      <td>Positiva</td>\n",
       "      <td>1e020d17-fb5c-4052-8774-67b431cda161</td>\n",
       "      <td>Negativa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>oWw-sNX79i</td>\n",
       "      <td></td>\n",
       "      <td>valorizacao dolar frente real elevado forca pr...</td>\n",
       "      <td>SOJA/CEPEA: Dólar sobe e eleva média mensal do...</td>\n",
       "      <td>Positiva</td>\n",
       "      <td>7fa6a610-b6e9-4abf-95c5-22ba119630de</td>\n",
       "      <td>Negativa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>TndmtqGgQA</td>\n",
       "      <td>2/10/2017</td>\n",
       "      <td>precos soja derivados subiram brasil setembro ...</td>\n",
       "      <td>SOJA/CEPEA: Demanda externa elevada e recuo pr...</td>\n",
       "      <td>Positiva</td>\n",
       "      <td>6ef899b3-cd6b-4e8d-8675-3a8e9c2db44e</td>\n",
       "      <td>Positiva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>67nRmTzCS7</td>\n",
       "      <td></td>\n",
       "      <td>menor demanda externa cautela compradores dome...</td>\n",
       "      <td>SOJA/CEPEA: Com menor procura e frete rodoviár...</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>4a09d340-1854-4968-a92c-fb5de9599843</td>\n",
       "      <td>Positiva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>qUVxbRQnyQ</td>\n",
       "      <td>08/05/2017</td>\n",
       "      <td>fim colheita safra nacional recorde precos soj...</td>\n",
       "      <td>SOJA/CEPEA: Cotações avançam pela 4º semana co...</td>\n",
       "      <td>Positiva</td>\n",
       "      <td>b7430298-2d42-4188-9fbc-48f9fc2c544a</td>\n",
       "      <td>Positiva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>S5JS_zabnl</td>\n",
       "      <td>26/10/2015</td>\n",
       "      <td>valorizacao dolar ultima semana produtores vol...</td>\n",
       "      <td>SOJA/CEPEA: Preços sobem; clima melhora e favo...</td>\n",
       "      <td>Positiva</td>\n",
       "      <td>70468e40-8c29-4681-909a-29c200fa6b52</td>\n",
       "      <td>Positiva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>buwb5cnqIJ</td>\n",
       "      <td>15/02/2017</td>\n",
       "      <td>precos soja derivados cairam brasil ultimos di...</td>\n",
       "      <td>SOJA/CEPEA: Queda nos preços é limitada por dó...</td>\n",
       "      <td>Neutra</td>\n",
       "      <td>6659f9f6-b7a5-4b2c-a322-b3fca135c13a</td>\n",
       "      <td>Negativa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>tXu9TREh_3</td>\n",
       "      <td>20/03/2023</td>\n",
       "      <td>volume chuva reduziu ultimos dias colheita soj...</td>\n",
       "      <td>SOJA/CEPEA: Volume de chuvas diminui, e colhei...</td>\n",
       "      <td>Positiva</td>\n",
       "      <td>2ba6bbf5-508b-4c40-9b2d-1eaa16390fcf</td>\n",
       "      <td>Negativa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id        data   \n",
       "246  9jPu-wLJ6L  27/05/2019  \\\n",
       "247  ASTg8bJjN5               \n",
       "249  NVXB7cSsqU  17/10/2022   \n",
       "250  oWw-sNX79i               \n",
       "251  TndmtqGgQA   2/10/2017   \n",
       "..          ...         ...   \n",
       "433  67nRmTzCS7               \n",
       "446  qUVxbRQnyQ  08/05/2017   \n",
       "449  S5JS_zabnl  26/10/2015   \n",
       "460  buwb5cnqIJ  15/02/2017   \n",
       "467  tXu9TREh_3  20/03/2023   \n",
       "\n",
       "                                               noticia   \n",
       "246  maiores demandas interna externa farelo oleo s...  \\\n",
       "247  boa parte safra soja brasileira ja comercializ...   \n",
       "249  precos soja voltaram subir mercado interno sem...   \n",
       "250  valorizacao dolar frente real elevado forca pr...   \n",
       "251  precos soja derivados subiram brasil setembro ...   \n",
       "..                                                 ...   \n",
       "433  menor demanda externa cautela compradores dome...   \n",
       "446  fim colheita safra nacional recorde precos soj...   \n",
       "449  valorizacao dolar ultima semana produtores vol...   \n",
       "460  precos soja derivados cairam brasil ultimos di...   \n",
       "467  volume chuva reduziu ultimos dias colheita soj...   \n",
       "\n",
       "                                                titulo   choices   \n",
       "246  SOJA/CEPEA: Demanda aquecida por farelo e óleo...  Positiva  \\\n",
       "247  SOJA/CEPEA: Com menor ritmo de embarques, prêm...  Negativa   \n",
       "249               SOJA/CEPEA: Valores reagem no Brasil  Positiva   \n",
       "250  SOJA/CEPEA: Dólar sobe e eleva média mensal do...  Positiva   \n",
       "251  SOJA/CEPEA: Demanda externa elevada e recuo pr...  Positiva   \n",
       "..                                                 ...       ...   \n",
       "433  SOJA/CEPEA: Com menor procura e frete rodoviár...  Negativa   \n",
       "446  SOJA/CEPEA: Cotações avançam pela 4º semana co...  Positiva   \n",
       "449  SOJA/CEPEA: Preços sobem; clima melhora e favo...  Positiva   \n",
       "460  SOJA/CEPEA: Queda nos preços é limitada por dó...    Neutra   \n",
       "467  SOJA/CEPEA: Volume de chuvas diminui, e colhei...  Positiva   \n",
       "\n",
       "                                unique_id  predicao  \n",
       "246  701c2b37-164f-4dc7-980c-e81234625d92  Positiva  \n",
       "247  85ba023a-e6c6-4768-967a-58d52de40d92  Negativa  \n",
       "249  1e020d17-fb5c-4052-8774-67b431cda161  Negativa  \n",
       "250  7fa6a610-b6e9-4abf-95c5-22ba119630de  Negativa  \n",
       "251  6ef899b3-cd6b-4e8d-8675-3a8e9c2db44e  Positiva  \n",
       "..                                    ...       ...  \n",
       "433  4a09d340-1854-4968-a92c-fb5de9599843  Positiva  \n",
       "446  b7430298-2d42-4188-9fbc-48f9fc2c544a  Positiva  \n",
       "449  70468e40-8c29-4681-909a-29c200fa6b52  Positiva  \n",
       "460  6659f9f6-b7a5-4b2c-a322-b3fca135c13a  Negativa  \n",
       "467  2ba6bbf5-508b-4c40-9b2d-1eaa16390fcf  Negativa  \n",
       "\n",
       "[84 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## criar uma tabela de predições\n",
    "df_predicoes = df_validacao.copy()\n",
    "df_predicoes['predicao'] = model.predict(padded_test_sequences)\n",
    "df_predicoes['predicao'] = df_predicoes['predicao'].apply(lambda x: 'Positiva' if x > 0.5 else 'Negativa' if x < 0.5 else 'Neutra')\n",
    "df_predicoes\n",
    "\n",
    "##  quantas noticias foram classificadas como positivas, negativas e neutras\n",
    "df_predicoes['predicao'].value_counts()\n",
    "\n",
    "## quantas noticias foram classificadas erradas\n",
    "df_predicoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "Positiva\n"
     ]
    }
   ],
   "source": [
    "## prever uma noticia\n",
    "df_validacao = df_validacao.reset_index(drop=True)\n",
    "noticia = df_validacao['noticia'][0]\n",
    "noticia = preprocess_text(noticia)\n",
    "noticia = [noticia]\n",
    "noticia = tokenizer.texts_to_sequences(noticia)\n",
    "noticia = pad_sequences(noticia, maxlen=max_length, padding='post', truncating='post')\n",
    "val = model.predict(noticia)[0][0]\n",
    "\n",
    "\n",
    "if val > 0.5:\n",
    "    print('Positiva')\n",
    "elif val < 0.5:\n",
    "    print('Negativa')\n",
    "else:\n",
    "    print('Neutra')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(29794, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Carregar o modelo pré-treinado BERTimbau\n",
    "model = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "\n",
    "model\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "# # Definir otimizador e função de perda\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# train_sentences = df_treino['noticia'].tolist()\n",
    "# train_labels = df_treino['choices'].map({'Positiva': 1, 'Negativa': 0, 'Neutra': 2}).tolist()\n",
    "# test_sentences = df_validacao['noticia'].tolist()\n",
    "# test_labels = df_validacao['choices'].map({'Positiva': 1, 'Negativa': 0, 'Neutra': 2}).tolist()\n",
    "\n",
    "# # Tokenizar as sentenças\n",
    "# train_encodings = tokenizer(train_sentences, truncation=True, padding=True, return_tensors='pt')\n",
    "# test_encodings = tokenizer(test_sentences, truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "# # Criar um dataset do pytorch\n",
    "# class MyDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, encodings, labels):\n",
    "#         self.encodings = encodings\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "#         item['labels'] = torch.tensor(self.labels[idx])\n",
    "#         return item\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "# train_dataset = MyDataset(train_encodings, train_labels)\n",
    "# test_dataset = MyDataset(test_encodings, test_labels)\n",
    "\n",
    "# # Treinar o modelo\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "# for epoch in range(2):\n",
    "#     for i, batch in enumerate(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         loss = outputs[0]\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# # Avaliar o modelo nos dados de teste\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for batch in test_loader:\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask)\n",
    "#         predictions = torch.argmax(outputs[0], axis=1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predictions == labels).sum().item()\n",
    "#     accuracy = correct / total\n",
    "#     print('Test accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdgd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
