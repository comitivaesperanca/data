{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file_upload                 236\n",
       "drafts                      236\n",
       "predictions                 236\n",
       "meta                        236\n",
       "created_at                  236\n",
       "updated_at                  236\n",
       "inner_id                    236\n",
       "total_annotations           236\n",
       "cancelled_annotations       236\n",
       "total_predictions           236\n",
       "comment_count               236\n",
       "unresolved_comment_count    236\n",
       "last_comment_updated_at       0\n",
       "project                     236\n",
       "updated_by                  236\n",
       "comment_authors             236\n",
       "completed_by                236\n",
       "was_cancelled               236\n",
       "ground_truth                236\n",
       "created_at                  236\n",
       "updated_at                  236\n",
       "lead_time                   236\n",
       "prediction                  236\n",
       "result_count                236\n",
       "unique_id                   236\n",
       "last_action                   0\n",
       "task                        236\n",
       "project                     236\n",
       "updated_by                  236\n",
       "parent_prediction             0\n",
       "parent_annotation             0\n",
       "last_created_by               0\n",
       "id                          236\n",
       "type                        236\n",
       "origin                      236\n",
       "to_name                     236\n",
       "from_name                   236\n",
       "0                             0\n",
       "choices                     236\n",
       "0                             0\n",
       "ano                         236\n",
       "mes                         236\n",
       "url                         236\n",
       "data                        236\n",
       "titulo                      236\n",
       "noticia                     236\n",
       "data_hora                   236\n",
       "Unnamed: 0                  236\n",
       "qtde_palavras_soja          236\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from unidecode import unidecode\n",
    "## ler arquivo json em pandas\n",
    "df = pd.read_json('/home/mfelipemota/projects/pantanaldev/data/label-studio/data/export/project-1-at-2023-04-30-05-08-956f4e3c.json')\n",
    "\n",
    "df = df.drop(['id'], axis=1)\n",
    "## expandir coluna annotations\n",
    "df = pd.concat([df.drop(['annotations'], axis=1), df['annotations'].apply(pd.Series)], axis=1)\n",
    "## expandir coluna 0 e renomear para annotations\n",
    "df = pd.concat([df.drop([0], axis=1), df[0].apply(pd.Series)], axis=1)\n",
    "## expandir coluna result e renomear para result\n",
    "df = pd.concat([df.drop(['result'], axis=1), df['result'].apply(pd.Series)], axis=1)\n",
    "df = df.drop(['id'], axis=1)\n",
    "\n",
    "## expandir coluna 0 e renomear para result\n",
    "df = pd.concat([df.drop([0], axis=1), df[0].apply(pd.Series)], axis=1)\n",
    "## expandir coluna value e renomear para value\n",
    "df = pd.concat([df.drop(['value'], axis=1), df['value'].apply(pd.Series)], axis=1)\n",
    "## dropar choices nulos\n",
    "df = df.dropna(subset=['choices'])\n",
    "## obter choices \n",
    "df['choices'] = df['choices'].apply(lambda x: x[0])\n",
    "\n",
    "## expandir coluna data\n",
    "df = pd.concat([df.drop(['data'], axis=1), df['data'].apply(pd.Series)], axis=1)\n",
    "\n",
    "df_noticia_original = df.copy()\n",
    "\n",
    "\n",
    "padrao_data_cepea = r\"Cepea, \\d{2}/\\d{2}/\\d{4} - \"\n",
    "df['noticia'] = df['noticia'].apply(lambda x: re.sub(padrao_data_cepea, '', x))\n",
    "\n",
    "## remover a palavra 'cepea' das noticias\n",
    "padrao_cepea = r\"Cepea\"\n",
    "df['noticia'] = df['noticia'].apply(lambda x: re.sub(padrao_cepea, '', x, flags=re.IGNORECASE))\n",
    "\n",
    "## remover numeros das noticias\n",
    "padrao_numeros = r'[0-9]+'\n",
    "df['noticia'] = df['noticia'].apply(lambda x: re.sub(padrao_numeros, '', x))\n",
    "\n",
    "## noticia que contem a palavra 'soja'\n",
    "df = df[df['titulo'].str.contains('soja', flags=re.IGNORECASE)]\n",
    "\n",
    "## remover noticias com choice 'desclassificar'\n",
    "df = df[df['choices'] != 'Desclassificar']\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 21:29:40.760916: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-06 21:29:40.804449: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-06 21:29:41.797710: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 1s 48ms/step - loss: 0.6924 - accuracy: 0.4606 - val_loss: 0.6922 - val_accuracy: 0.4366\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.6901 - accuracy: 0.4667 - val_loss: 0.6913 - val_accuracy: 0.4366\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.6867 - accuracy: 0.4667 - val_loss: 0.6904 - val_accuracy: 0.4366\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.6829 - accuracy: 0.4667 - val_loss: 0.6894 - val_accuracy: 0.4366\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.6784 - accuracy: 0.4667 - val_loss: 0.6879 - val_accuracy: 0.4366\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.6725 - accuracy: 0.4667 - val_loss: 0.6858 - val_accuracy: 0.4366\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.6651 - accuracy: 0.4667 - val_loss: 0.6822 - val_accuracy: 0.4366\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.6568 - accuracy: 0.5091 - val_loss: 0.6775 - val_accuracy: 0.4366\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.6444 - accuracy: 0.5455 - val_loss: 0.6712 - val_accuracy: 0.4366\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.6269 - accuracy: 0.5697 - val_loss: 0.6626 - val_accuracy: 0.4507\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.6054 - accuracy: 0.6000 - val_loss: 0.6537 - val_accuracy: 0.4648\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.5814 - accuracy: 0.6606 - val_loss: 0.6397 - val_accuracy: 0.5493\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.5522 - accuracy: 0.6667 - val_loss: 0.6258 - val_accuracy: 0.5493\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.5173 - accuracy: 0.7091 - val_loss: 0.6065 - val_accuracy: 0.6197\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.4833 - accuracy: 0.7576 - val_loss: 0.5893 - val_accuracy: 0.6620\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4459 - accuracy: 0.7515 - val_loss: 0.5707 - val_accuracy: 0.6197\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.4000 - accuracy: 0.7879 - val_loss: 0.5662 - val_accuracy: 0.6479\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.3660 - accuracy: 0.8061 - val_loss: 0.5396 - val_accuracy: 0.6761\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.3247 - accuracy: 0.8000 - val_loss: 0.5291 - val_accuracy: 0.6761\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.2899 - accuracy: 0.8061 - val_loss: 0.5273 - val_accuracy: 0.6338\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.2565 - accuracy: 0.8061 - val_loss: 0.5180 - val_accuracy: 0.6479\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.2282 - accuracy: 0.8061 - val_loss: 0.5248 - val_accuracy: 0.6338\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.2071 - accuracy: 0.8061 - val_loss: 0.5182 - val_accuracy: 0.6620\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.1938 - accuracy: 0.8061 - val_loss: 0.5325 - val_accuracy: 0.6479\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1801 - accuracy: 0.8061 - val_loss: 0.5381 - val_accuracy: 0.6338\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.1692 - accuracy: 0.8061 - val_loss: 0.5302 - val_accuracy: 0.6338\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.1624 - accuracy: 0.8061 - val_loss: 0.5575 - val_accuracy: 0.6479\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.1586 - accuracy: 0.8061 - val_loss: 0.5501 - val_accuracy: 0.6338\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1530 - accuracy: 0.8061 - val_loss: 0.5604 - val_accuracy: 0.6338\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1494 - accuracy: 0.8061 - val_loss: 0.5709 - val_accuracy: 0.6338\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.1472 - accuracy: 0.8061 - val_loss: 0.5691 - val_accuracy: 0.6338\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.1454 - accuracy: 0.8061 - val_loss: 0.5819 - val_accuracy: 0.6338\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1439 - accuracy: 0.8061 - val_loss: 0.5818 - val_accuracy: 0.6338\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.1429 - accuracy: 0.8061 - val_loss: 0.6042 - val_accuracy: 0.6479\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1421 - accuracy: 0.8061 - val_loss: 0.5877 - val_accuracy: 0.6338\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.1415 - accuracy: 0.8061 - val_loss: 0.6233 - val_accuracy: 0.6479\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1414 - accuracy: 0.8061 - val_loss: 0.6005 - val_accuracy: 0.6338\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1405 - accuracy: 0.8061 - val_loss: 0.6178 - val_accuracy: 0.6338\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.1399 - accuracy: 0.8061 - val_loss: 0.6212 - val_accuracy: 0.6338\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1397 - accuracy: 0.8061 - val_loss: 0.6357 - val_accuracy: 0.6338\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1393 - accuracy: 0.8061 - val_loss: 0.6177 - val_accuracy: 0.6338\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1394 - accuracy: 0.8061 - val_loss: 0.6299 - val_accuracy: 0.6338\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1394 - accuracy: 0.8061 - val_loss: 0.6412 - val_accuracy: 0.6338\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.1391 - accuracy: 0.8061 - val_loss: 0.6250 - val_accuracy: 0.6479\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1384 - accuracy: 0.8061 - val_loss: 0.6479 - val_accuracy: 0.6338\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1379 - accuracy: 0.8061 - val_loss: 0.6380 - val_accuracy: 0.6338\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.1382 - accuracy: 0.8061 - val_loss: 0.6432 - val_accuracy: 0.6338\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1379 - accuracy: 0.8061 - val_loss: 0.6610 - val_accuracy: 0.6338\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1378 - accuracy: 0.8061 - val_loss: 0.6410 - val_accuracy: 0.6479\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1371 - accuracy: 0.8061 - val_loss: 0.6825 - val_accuracy: 0.6338\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1381 - accuracy: 0.8061 - val_loss: 0.6499 - val_accuracy: 0.6338\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.1382 - accuracy: 0.8061 - val_loss: 0.6617 - val_accuracy: 0.6338\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.1386 - accuracy: 0.8061 - val_loss: 0.7115 - val_accuracy: 0.6479\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1453 - accuracy: 0.8061 - val_loss: 0.6373 - val_accuracy: 0.6479\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1456 - accuracy: 0.8061 - val_loss: 0.6709 - val_accuracy: 0.6338\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1484 - accuracy: 0.8061 - val_loss: 0.6863 - val_accuracy: 0.6338\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1543 - accuracy: 0.8061 - val_loss: 0.6275 - val_accuracy: 0.6479\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1532 - accuracy: 0.8061 - val_loss: 0.7121 - val_accuracy: 0.6479\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1456 - accuracy: 0.8061 - val_loss: 0.6301 - val_accuracy: 0.6338\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.1497 - accuracy: 0.8061 - val_loss: 0.7014 - val_accuracy: 0.6338\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1452 - accuracy: 0.8061 - val_loss: 0.6244 - val_accuracy: 0.6479\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1439 - accuracy: 0.8061 - val_loss: 0.7154 - val_accuracy: 0.6338\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1486 - accuracy: 0.8061 - val_loss: 0.6244 - val_accuracy: 0.6479\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1489 - accuracy: 0.8061 - val_loss: 0.7304 - val_accuracy: 0.6338\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.1485 - accuracy: 0.8061 - val_loss: 0.6272 - val_accuracy: 0.6479\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1453 - accuracy: 0.8061 - val_loss: 0.7384 - val_accuracy: 0.6338\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1508 - accuracy: 0.8061 - val_loss: 0.6290 - val_accuracy: 0.6479\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.1511 - accuracy: 0.8061 - val_loss: 0.6811 - val_accuracy: 0.6338\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.1477 - accuracy: 0.8061 - val_loss: 0.6441 - val_accuracy: 0.6479\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1501 - accuracy: 0.8061 - val_loss: 0.6600 - val_accuracy: 0.6338\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1469 - accuracy: 0.8061 - val_loss: 0.6421 - val_accuracy: 0.6479\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1462 - accuracy: 0.8061 - val_loss: 0.6478 - val_accuracy: 0.6338\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.1406 - accuracy: 0.8061 - val_loss: 0.6756 - val_accuracy: 0.6338\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.1400 - accuracy: 0.8061 - val_loss: 0.6357 - val_accuracy: 0.6479\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1382 - accuracy: 0.8061 - val_loss: 0.7031 - val_accuracy: 0.6338\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.1411 - accuracy: 0.8061 - val_loss: 0.6431 - val_accuracy: 0.6338\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.1395 - accuracy: 0.8061 - val_loss: 0.6812 - val_accuracy: 0.6338\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.1402 - accuracy: 0.8061 - val_loss: 0.6527 - val_accuracy: 0.6479\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1390 - accuracy: 0.8061 - val_loss: 0.6687 - val_accuracy: 0.6338\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.1373 - accuracy: 0.8061 - val_loss: 0.6604 - val_accuracy: 0.6338\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.1395 - accuracy: 0.8061 - val_loss: 0.6683 - val_accuracy: 0.6338\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.1397 - accuracy: 0.8061 - val_loss: 0.6606 - val_accuracy: 0.6338\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1382 - accuracy: 0.8061 - val_loss: 0.6717 - val_accuracy: 0.6338\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1380 - accuracy: 0.8061 - val_loss: 0.6670 - val_accuracy: 0.6338\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1368 - accuracy: 0.8061 - val_loss: 0.6677 - val_accuracy: 0.6338\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1363 - accuracy: 0.8061 - val_loss: 0.6771 - val_accuracy: 0.6338\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1362 - accuracy: 0.8061 - val_loss: 0.6696 - val_accuracy: 0.6338\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1362 - accuracy: 0.8061 - val_loss: 0.6761 - val_accuracy: 0.6338\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1362 - accuracy: 0.8061 - val_loss: 0.6772 - val_accuracy: 0.6338\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.1363 - accuracy: 0.8061 - val_loss: 0.6747 - val_accuracy: 0.6338\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1361 - accuracy: 0.8061 - val_loss: 0.6829 - val_accuracy: 0.6338\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1359 - accuracy: 0.8061 - val_loss: 0.6816 - val_accuracy: 0.6338\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.1358 - accuracy: 0.8061 - val_loss: 0.6891 - val_accuracy: 0.6338\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1358 - accuracy: 0.8061 - val_loss: 0.6856 - val_accuracy: 0.6338\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1357 - accuracy: 0.8061 - val_loss: 0.6914 - val_accuracy: 0.6338\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.1356 - accuracy: 0.8061 - val_loss: 0.6890 - val_accuracy: 0.6338\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1356 - accuracy: 0.8061 - val_loss: 0.6963 - val_accuracy: 0.6338\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1356 - accuracy: 0.8061 - val_loss: 0.6917 - val_accuracy: 0.6338\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1356 - accuracy: 0.8061 - val_loss: 0.7009 - val_accuracy: 0.6338\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1355 - accuracy: 0.8061 - val_loss: 0.6951 - val_accuracy: 0.6338\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6104 - accuracy: 0.6923\n",
      "Test accuracy: 0.692307710647583\n"
     ]
    }
   ],
   "source": [
    "# Selecionar apenas as colunas necessárias\n",
    "columns_to_select = ['id', 'data', 'noticia', 'titulo', 'choices', 'unique_id']\n",
    "\n",
    "df = df[columns_to_select]\n",
    "df.dropna(subset=['noticia'])\n",
    "\n",
    "# Pré-processamento dos dados\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # remover acentuação\n",
    "    text = unidecode(text)\n",
    "    # Remover pontuações\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenização\n",
    "    words = word_tokenize(text.lower())\n",
    "    # Remover stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['noticia'] = df['noticia'].apply(preprocess_text)\n",
    "\n",
    "df_treino = df[:210]\n",
    "df_validacao = df[210:]\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Criar modelo de classificação de sentimento\n",
    "vocab_size = 1000\n",
    "embedding_dim = 100\n",
    "max_length = 125\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dense(50, activation='relu'),\n",
    "    keras.layers.Dense(25, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Pré-processar os dados\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df['noticia'])\n",
    "sequences = tokenizer.texts_to_sequences(df['noticia'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "labels = df['choices'].map({'Positiva': 1, 'Negativa': 0, 'Neutra': 0.5})\n",
    "\n",
    "# Treinar o modelo\n",
    "history = model.fit(padded_sequences, labels, epochs=100, validation_split=0.3)\n",
    "\n",
    "# Avaliar o modelo\n",
    "test_sequences = tokenizer.texts_to_sequences(df_validacao['noticia'])\n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "test_labels = df_validacao['choices'].map({'Positiva': 1, 'Negativa': 0, 'Neutra': 0.5})\n",
    "test_loss, test_accuracy = model.evaluate(padded_test_sequences, test_labels)\n",
    "print('Test accuracy:', test_accuracy)\n",
    "\n",
    "# Salvar o modelo\n",
    "model.save('modelo.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 109ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>data</th>\n",
       "      <th>noticia</th>\n",
       "      <th>titulo</th>\n",
       "      <th>choices</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>predicao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>pQtBUDUD6I</td>\n",
       "      <td>4/10/2010</td>\n",
       "      <td>chuvas registradas boa parte pais ultimos dias...</td>\n",
       "      <td>SOJA/CEPEA: Clima seco em MT ainda preocupa</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>e183012a-c39b-4609-8901-b168969ee53a</td>\n",
       "      <td>Negativa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>wg007o9U_l</td>\n",
       "      <td></td>\n",
       "      <td>precos soja derivados cairam mercado brasileir...</td>\n",
       "      <td>SOJA/CEPEA: Preços caem mesmo com atraso da co...</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>a240499e-c9c9-47ea-9978-12cdb0ef9a2d</td>\n",
       "      <td>Negativa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>cLvxr6IWgx</td>\n",
       "      <td>19/03/2007</td>\n",
       "      <td>indicador soja esalq mercado lotes parana caiu...</td>\n",
       "      <td>SOJA: Indicador cai 2% em sete dias</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>d09539ea-f73c-4a11-a2c2-92c892e45763</td>\n",
       "      <td>Negativa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>uUNzG2Aq7t</td>\n",
       "      <td>08/01/2007</td>\n",
       "      <td>chuvas principais regioes produtoras soja bras...</td>\n",
       "      <td>SOJA: Clima beneficia lavouras</td>\n",
       "      <td>Positiva</td>\n",
       "      <td>d6e77e6e-427a-4d61-8106-9a0694381a1a</td>\n",
       "      <td>Negativa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>mK2V-OT8b8</td>\n",
       "      <td>22/01/2007</td>\n",
       "      <td>precos internos soja subiram ultimos dias inic...</td>\n",
       "      <td>SOJA: Prêmios limitam alta de preços</td>\n",
       "      <td>Positiva</td>\n",
       "      <td>0b135750-bc28-4a0d-a9de-aa82737b917f</td>\n",
       "      <td>Positiva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>-hivWwwlgA</td>\n",
       "      <td>04/12/2006</td>\n",
       "      <td>daqui pra frente produtores devem focados clim...</td>\n",
       "      <td>SOJA:  Fitossanidade preocupa setor</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>7b63f020-2227-4618-988a-bacc86185f2f</td>\n",
       "      <td>Negativa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>MnN7rKLL5W</td>\n",
       "      <td>02/01/2019</td>\n",
       "      <td>temporada soja registrando producao superior a...</td>\n",
       "      <td>SOJA/CEPEA: Preços se sustentam ao longo de 2018</td>\n",
       "      <td>Neutra</td>\n",
       "      <td>5e83462c-babc-4838-9624-dcd7dc44c54b</td>\n",
       "      <td>Negativa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>rvYNosmiR-</td>\n",
       "      <td>26/12/2018</td>\n",
       "      <td>ainda ritmo lento parte produtores regiao nort...</td>\n",
       "      <td>SOJA/CEPEA: Colheita tem início em MT e deve s...</td>\n",
       "      <td>Neutra</td>\n",
       "      <td>2eb88c4f-2710-4b2b-82f4-cd8f3f9186ce</td>\n",
       "      <td>Positiva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>ksRGb9gGig</td>\n",
       "      <td>30/07/2018</td>\n",
       "      <td>precos soja estao queda mercado brasileiro aco...</td>\n",
       "      <td>SOJA/CEPEA: Com baixa liquidez, preços caem no...</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>b91d6110-b000-46f6-afea-790d74223b1e</td>\n",
       "      <td>Negativa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>rASALqlqS3</td>\n",
       "      <td>06/03/2019</td>\n",
       "      <td>recentes chuvas atrapalhado trabalhos campo ge...</td>\n",
       "      <td>SOJA/CEPEA: Caso persista, clima chuvoso pode ...</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>0f0f27f4-2a4c-4588-b6c3-d127fe776aa0</td>\n",
       "      <td>Negativa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>Rv1vW3I1uz</td>\n",
       "      <td>01/04/2019</td>\n",
       "      <td>precos soja subiram mercado brasileiro marco e...</td>\n",
       "      <td>SOJA/CEPEA: Dólar sobe, incentiva novas vendas...</td>\n",
       "      <td>Positiva</td>\n",
       "      <td>b7ff7e55-f11d-452b-a1f4-86dc69a085f2</td>\n",
       "      <td>Positiva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>aV7UyK4RZM</td>\n",
       "      <td>21/11/2022</td>\n",
       "      <td>comercializacao soja grao aquecida mercado bra...</td>\n",
       "      <td>SOJA/CEPEA: Negócios estão aquecidos no Brasil</td>\n",
       "      <td>Neutra</td>\n",
       "      <td>d33c6fb5-ed94-4a4c-b0e4-3128cd190b1f</td>\n",
       "      <td>Positiva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>LOGA1ClkXg</td>\n",
       "      <td>14/06/2021</td>\n",
       "      <td>precos soja cairam brasil estados unidos longo...</td>\n",
       "      <td>SOJA/CEPEA: Estimativa de maior estoque global...</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>5bdafa16-a008-4c64-b8a5-78816dc6e1e0</td>\n",
       "      <td>Negativa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>JHcB5RLVwN</td>\n",
       "      <td></td>\n",
       "      <td>liquidez mercado soja derivados segue baixa ta...</td>\n",
       "      <td>SOJA/CEPEA: Mercado incerto e disparidade entr...</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>8e64b183-94a8-4dc6-9738-9be2d51b6950</td>\n",
       "      <td>Negativa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>IV7mOWePyt</td>\n",
       "      <td>29/03/2021</td>\n",
       "      <td>precos soja subiram mercado brasileiro semana ...</td>\n",
       "      <td>SOJA/CEPEA: Produtor se retrai e valores volta...</td>\n",
       "      <td>Positiva</td>\n",
       "      <td>5c78ea67-2c29-4fb1-81b6-b509bc917aa7</td>\n",
       "      <td>Positiva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>MAz7mWBrbo</td>\n",
       "      <td>03/05/2021</td>\n",
       "      <td>precos soja recuaram mercado brasileiro semana...</td>\n",
       "      <td>SOJA/CEPEA: Desvalorização do dólar enfraquece...</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>ac90e322-0faa-42dd-a234-4c9af20b81bf</td>\n",
       "      <td>Negativa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>SiFeQiICA5</td>\n",
       "      <td>09/05/2022</td>\n",
       "      <td>precos soja registraram baixa ultimos dias seg...</td>\n",
       "      <td>SOJA/CEPEA: Com avanço da colheita e baixa dem...</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>c5dbacbf-b29b-4c63-b2ec-39052f1faacc</td>\n",
       "      <td>Negativa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>Is2n7pZmY5</td>\n",
       "      <td></td>\n",
       "      <td>enquanto precos estao estaveis mercado interna...</td>\n",
       "      <td>SOJA/CEPEA: Desvalorização do dólar enfraquece...</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>c6058860-d5a7-4179-ada3-9edd9f883e1d</td>\n",
       "      <td>Negativa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>eAi4i5EwEM</td>\n",
       "      <td>18/11/2013</td>\n",
       "      <td>precos soja derivados registrado alta praticam...</td>\n",
       "      <td>SOJA/CEPEA: Cotações internas do grão e dos de...</td>\n",
       "      <td>Positiva</td>\n",
       "      <td>6d10b871-79f6-4838-a47a-e0fe0e3aaa44</td>\n",
       "      <td>Positiva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>PHAzgLtJaF</td>\n",
       "      <td>16/12/2013</td>\n",
       "      <td>aproximacao final ano negociacoes envolvendo s...</td>\n",
       "      <td>SOJA/CEPEA: Setor se atenta ao desenvolvimento...</td>\n",
       "      <td>Neutra</td>\n",
       "      <td>fc9555ae-6247-45f3-b283-3a2106375883</td>\n",
       "      <td>Negativa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>c2eTyfZAH8</td>\n",
       "      <td>4/06/2012</td>\n",
       "      <td>vendedores soja resistido propostas compradore...</td>\n",
       "      <td>SOJA/CEPEA: Vendedores se retraem e negócios d...</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>23ac78e4-686f-4110-b88f-71a5082ccd47</td>\n",
       "      <td>Negativa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>ZMhiSD6Rk2</td>\n",
       "      <td>12/03/2012</td>\n",
       "      <td>segundo dados precos soja brasil continuaram f...</td>\n",
       "      <td>SOJA/CEPEA: Demanda prevalece sobre oferta e p...</td>\n",
       "      <td>Positiva</td>\n",
       "      <td>bf6497cb-2e72-46da-9942-5be2f7822611</td>\n",
       "      <td>Positiva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>67nRmTzCS7</td>\n",
       "      <td></td>\n",
       "      <td>menor demanda externa cautela compradores dome...</td>\n",
       "      <td>SOJA/CEPEA: Com menor procura e frete rodoviár...</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>4a09d340-1854-4968-a92c-fb5de9599843</td>\n",
       "      <td>Positiva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>S5JS_zabnl</td>\n",
       "      <td>26/10/2015</td>\n",
       "      <td>valorizacao dolar ultima semana produtores vol...</td>\n",
       "      <td>SOJA/CEPEA: Preços sobem; clima melhora e favo...</td>\n",
       "      <td>Positiva</td>\n",
       "      <td>70468e40-8c29-4681-909a-29c200fa6b52</td>\n",
       "      <td>Positiva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>buwb5cnqIJ</td>\n",
       "      <td>15/02/2017</td>\n",
       "      <td>precos soja derivados cairam brasil ultimos di...</td>\n",
       "      <td>SOJA/CEPEA: Queda nos preços é limitada por dó...</td>\n",
       "      <td>Neutra</td>\n",
       "      <td>6659f9f6-b7a5-4b2c-a322-b3fca135c13a</td>\n",
       "      <td>Negativa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>tXu9TREh_3</td>\n",
       "      <td>20/03/2023</td>\n",
       "      <td>volume chuva reduziu ultimos dias colheita soj...</td>\n",
       "      <td>SOJA/CEPEA: Volume de chuvas diminui, e colhei...</td>\n",
       "      <td>Positiva</td>\n",
       "      <td>2ba6bbf5-508b-4c40-9b2d-1eaa16390fcf</td>\n",
       "      <td>Negativa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id        data   \n",
       "257  pQtBUDUD6I   4/10/2010  \\\n",
       "263  wg007o9U_l               \n",
       "265  cLvxr6IWgx  19/03/2007   \n",
       "266  uUNzG2Aq7t  08/01/2007   \n",
       "267  mK2V-OT8b8  22/01/2007   \n",
       "269  -hivWwwlgA  04/12/2006   \n",
       "273  MnN7rKLL5W  02/01/2019   \n",
       "276  rvYNosmiR-  26/12/2018   \n",
       "279  ksRGb9gGig  30/07/2018   \n",
       "286  rASALqlqS3  06/03/2019   \n",
       "292  Rv1vW3I1uz  01/04/2019   \n",
       "295  aV7UyK4RZM  21/11/2022   \n",
       "301  LOGA1ClkXg  14/06/2021   \n",
       "302  JHcB5RLVwN               \n",
       "303  IV7mOWePyt  29/03/2021   \n",
       "312  MAz7mWBrbo  03/05/2021   \n",
       "316  SiFeQiICA5  09/05/2022   \n",
       "327  Is2n7pZmY5               \n",
       "329  eAi4i5EwEM  18/11/2013   \n",
       "330  PHAzgLtJaF  16/12/2013   \n",
       "332  c2eTyfZAH8   4/06/2012   \n",
       "335  ZMhiSD6Rk2  12/03/2012   \n",
       "340  67nRmTzCS7               \n",
       "352  S5JS_zabnl  26/10/2015   \n",
       "361  buwb5cnqIJ  15/02/2017   \n",
       "367  tXu9TREh_3  20/03/2023   \n",
       "\n",
       "                                               noticia   \n",
       "257  chuvas registradas boa parte pais ultimos dias...  \\\n",
       "263  precos soja derivados cairam mercado brasileir...   \n",
       "265  indicador soja esalq mercado lotes parana caiu...   \n",
       "266  chuvas principais regioes produtoras soja bras...   \n",
       "267  precos internos soja subiram ultimos dias inic...   \n",
       "269  daqui pra frente produtores devem focados clim...   \n",
       "273  temporada soja registrando producao superior a...   \n",
       "276  ainda ritmo lento parte produtores regiao nort...   \n",
       "279  precos soja estao queda mercado brasileiro aco...   \n",
       "286  recentes chuvas atrapalhado trabalhos campo ge...   \n",
       "292  precos soja subiram mercado brasileiro marco e...   \n",
       "295  comercializacao soja grao aquecida mercado bra...   \n",
       "301  precos soja cairam brasil estados unidos longo...   \n",
       "302  liquidez mercado soja derivados segue baixa ta...   \n",
       "303  precos soja subiram mercado brasileiro semana ...   \n",
       "312  precos soja recuaram mercado brasileiro semana...   \n",
       "316  precos soja registraram baixa ultimos dias seg...   \n",
       "327  enquanto precos estao estaveis mercado interna...   \n",
       "329  precos soja derivados registrado alta praticam...   \n",
       "330  aproximacao final ano negociacoes envolvendo s...   \n",
       "332  vendedores soja resistido propostas compradore...   \n",
       "335  segundo dados precos soja brasil continuaram f...   \n",
       "340  menor demanda externa cautela compradores dome...   \n",
       "352  valorizacao dolar ultima semana produtores vol...   \n",
       "361  precos soja derivados cairam brasil ultimos di...   \n",
       "367  volume chuva reduziu ultimos dias colheita soj...   \n",
       "\n",
       "                                                titulo   choices   \n",
       "257        SOJA/CEPEA: Clima seco em MT ainda preocupa  Negativa  \\\n",
       "263  SOJA/CEPEA: Preços caem mesmo com atraso da co...  Negativa   \n",
       "265                SOJA: Indicador cai 2% em sete dias  Negativa   \n",
       "266                     SOJA: Clima beneficia lavouras  Positiva   \n",
       "267               SOJA: Prêmios limitam alta de preços  Positiva   \n",
       "269                SOJA:  Fitossanidade preocupa setor  Negativa   \n",
       "273   SOJA/CEPEA: Preços se sustentam ao longo de 2018    Neutra   \n",
       "276  SOJA/CEPEA: Colheita tem início em MT e deve s...    Neutra   \n",
       "279  SOJA/CEPEA: Com baixa liquidez, preços caem no...  Negativa   \n",
       "286  SOJA/CEPEA: Caso persista, clima chuvoso pode ...  Negativa   \n",
       "292  SOJA/CEPEA: Dólar sobe, incentiva novas vendas...  Positiva   \n",
       "295     SOJA/CEPEA: Negócios estão aquecidos no Brasil    Neutra   \n",
       "301  SOJA/CEPEA: Estimativa de maior estoque global...  Negativa   \n",
       "302  SOJA/CEPEA: Mercado incerto e disparidade entr...  Negativa   \n",
       "303  SOJA/CEPEA: Produtor se retrai e valores volta...  Positiva   \n",
       "312  SOJA/CEPEA: Desvalorização do dólar enfraquece...  Negativa   \n",
       "316  SOJA/CEPEA: Com avanço da colheita e baixa dem...  Negativa   \n",
       "327  SOJA/CEPEA: Desvalorização do dólar enfraquece...  Negativa   \n",
       "329  SOJA/CEPEA: Cotações internas do grão e dos de...  Positiva   \n",
       "330  SOJA/CEPEA: Setor se atenta ao desenvolvimento...    Neutra   \n",
       "332  SOJA/CEPEA: Vendedores se retraem e negócios d...  Negativa   \n",
       "335  SOJA/CEPEA: Demanda prevalece sobre oferta e p...  Positiva   \n",
       "340  SOJA/CEPEA: Com menor procura e frete rodoviár...  Negativa   \n",
       "352  SOJA/CEPEA: Preços sobem; clima melhora e favo...  Positiva   \n",
       "361  SOJA/CEPEA: Queda nos preços é limitada por dó...    Neutra   \n",
       "367  SOJA/CEPEA: Volume de chuvas diminui, e colhei...  Positiva   \n",
       "\n",
       "                                unique_id  predicao  \n",
       "257  e183012a-c39b-4609-8901-b168969ee53a  Negativa  \n",
       "263  a240499e-c9c9-47ea-9978-12cdb0ef9a2d  Negativa  \n",
       "265  d09539ea-f73c-4a11-a2c2-92c892e45763  Negativa  \n",
       "266  d6e77e6e-427a-4d61-8106-9a0694381a1a  Negativa  \n",
       "267  0b135750-bc28-4a0d-a9de-aa82737b917f  Positiva  \n",
       "269  7b63f020-2227-4618-988a-bacc86185f2f  Negativa  \n",
       "273  5e83462c-babc-4838-9624-dcd7dc44c54b  Negativa  \n",
       "276  2eb88c4f-2710-4b2b-82f4-cd8f3f9186ce  Positiva  \n",
       "279  b91d6110-b000-46f6-afea-790d74223b1e  Negativa  \n",
       "286  0f0f27f4-2a4c-4588-b6c3-d127fe776aa0  Negativa  \n",
       "292  b7ff7e55-f11d-452b-a1f4-86dc69a085f2  Positiva  \n",
       "295  d33c6fb5-ed94-4a4c-b0e4-3128cd190b1f  Positiva  \n",
       "301  5bdafa16-a008-4c64-b8a5-78816dc6e1e0  Negativa  \n",
       "302  8e64b183-94a8-4dc6-9738-9be2d51b6950  Negativa  \n",
       "303  5c78ea67-2c29-4fb1-81b6-b509bc917aa7  Positiva  \n",
       "312  ac90e322-0faa-42dd-a234-4c9af20b81bf  Negativa  \n",
       "316  c5dbacbf-b29b-4c63-b2ec-39052f1faacc  Negativa  \n",
       "327  c6058860-d5a7-4179-ada3-9edd9f883e1d  Negativa  \n",
       "329  6d10b871-79f6-4838-a47a-e0fe0e3aaa44  Positiva  \n",
       "330  fc9555ae-6247-45f3-b283-3a2106375883  Negativa  \n",
       "332  23ac78e4-686f-4110-b88f-71a5082ccd47  Negativa  \n",
       "335  bf6497cb-2e72-46da-9942-5be2f7822611  Positiva  \n",
       "340  4a09d340-1854-4968-a92c-fb5de9599843  Positiva  \n",
       "352  70468e40-8c29-4681-909a-29c200fa6b52  Positiva  \n",
       "361  6659f9f6-b7a5-4b2c-a322-b3fca135c13a  Negativa  \n",
       "367  2ba6bbf5-508b-4c40-9b2d-1eaa16390fcf  Negativa  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## criar uma tabela de predições\n",
    "df_predicoes = df_validacao.copy()\n",
    "df_predicoes['predicao'] = model.predict(padded_test_sequences)\n",
    "df_predicoes['predicao'] = df_predicoes['predicao'].apply(lambda x: 'Positiva' if x > 0.5 else 'Negativa' if x < 0.5 else 'Neutra')\n",
    "df_predicoes\n",
    "\n",
    "##  quantas noticias foram classificadas como positivas, negativas e neutras\n",
    "df_predicoes['predicao'].value_counts()\n",
    "\n",
    "## quantas noticias foram classificadas erradas\n",
    "df_predicoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "Negativa\n"
     ]
    }
   ],
   "source": [
    "## prever uma noticia\n",
    "df_validacao = df_validacao.reset_index(drop=True)\n",
    "noticia = df_validacao['noticia'][0]\n",
    "noticia = preprocess_text(noticia)\n",
    "noticia = [noticia]\n",
    "noticia = tokenizer.texts_to_sequences(noticia)\n",
    "noticia = pad_sequences(noticia, maxlen=max_length, padding='post', truncating='post')\n",
    "val = model.predict(noticia)[0][0]\n",
    "\n",
    "\n",
    "if val > 0.5:\n",
    "    print('Positiva')\n",
    "elif val < 0.5:\n",
    "    print('Negativa')\n",
    "else:\n",
    "    print('Neutra')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/tmp/ipykernel_6141/2850895786.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Carregar o modelo pré-treinado BERTimbau\n",
    "model = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Definir otimizador e função de perda\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_sentences = df_treino['noticia'].tolist()\n",
    "train_labels = df_treino['choices'].map({'Positiva': 1, 'Negativa': 0, 'Neutra': 2}).tolist()\n",
    "test_sentences = df_validacao['noticia'].tolist()\n",
    "test_labels = df_validacao['choices'].map({'Positiva': 1, 'Negativa': 0, 'Neutra': 2}).tolist()\n",
    "\n",
    "# Tokenizar as sentenças\n",
    "train_encodings = tokenizer(train_sentences, truncation=True, padding=True, return_tensors='pt')\n",
    "test_encodings = tokenizer(test_sentences, truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "# Criar um dataset do pytorch\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = MyDataset(train_encodings, train_labels)\n",
    "test_dataset = MyDataset(test_encodings, test_labels)\n",
    "\n",
    "# Treinar o modelo\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "for epoch in range(2):\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Avaliar o modelo nos dados de teste\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs[0], axis=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print('Test accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdgd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
