{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from unidecode import unidecode\n",
    "## ler arquivo json em pandas\n",
    "df = pd.read_json('C:\\\\Users\\\\mayco\\\\Documents\\\\projetos\\\\data-pantanaldev\\\\label-studio\\\\data\\\\export\\\\project-1-at-2023-04-30-05-08-956f4e3c.json')\n",
    "\n",
    "df = df.drop(['id'], axis=1)\n",
    "## expandir coluna annotations\n",
    "df = pd.concat([df.drop(['annotations'], axis=1), df['annotations'].apply(pd.Series)], axis=1)\n",
    "## expandir coluna 0 e renomear para annotations\n",
    "df = pd.concat([df.drop([0], axis=1), df[0].apply(pd.Series)], axis=1)\n",
    "## expandir coluna result e renomear para result\n",
    "df = pd.concat([df.drop(['result'], axis=1), df['result'].apply(pd.Series)], axis=1)\n",
    "df = df.drop(['id'], axis=1)\n",
    "\n",
    "## expandir coluna 0 e renomear para result\n",
    "df = pd.concat([df.drop([0], axis=1), df[0].apply(pd.Series)], axis=1)\n",
    "## expandir coluna value e renomear para value\n",
    "df = pd.concat([df.drop(['value'], axis=1), df['value'].apply(pd.Series)], axis=1)\n",
    "## dropar choices nulos\n",
    "df = df.dropna(subset=['choices'])\n",
    "## obter choices \n",
    "df['choices'] = df['choices'].apply(lambda x: x[0])\n",
    "\n",
    "## expandir coluna data\n",
    "df = pd.concat([df.drop(['data'], axis=1), df['data'].apply(pd.Series)], axis=1)\n",
    "\n",
    "df_noticia_original = df.copy()\n",
    "\n",
    "\n",
    "padrao_data_cepea = r\"Cepea, \\d{2}/\\d{2}/\\d{4} - \"\n",
    "df['noticia'] = df['noticia'].apply(lambda x: re.sub(padrao_data_cepea, '', x))\n",
    "\n",
    "## remover a palavra 'cepea' das noticias\n",
    "padrao_cepea = r\"Cepea\"\n",
    "df['noticia'] = df['noticia'].apply(lambda x: re.sub(padrao_cepea, '', x, flags=re.IGNORECASE))\n",
    "\n",
    "## remover numeros das noticias\n",
    "padrao_numeros = r'[0-9]+'\n",
    "df['noticia'] = df['noticia'].apply(lambda x: re.sub(padrao_numeros, '', x))\n",
    "\n",
    "## noticia que contem a palavra 'soja'\n",
    "df = df[df['titulo'].str.contains('soja', flags=re.IGNORECASE)]\n",
    "\n",
    "## remover noticias com choice 'desclassificar'\n",
    "df = df[df['choices'] != 'Desclassificar']\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar apenas as colunas necessárias\n",
    "columns_to_select = ['id', 'data', 'noticia', 'titulo', 'choices', 'unique_id']\n",
    "\n",
    "df = df[columns_to_select]\n",
    "df.dropna(subset=['noticia'])\n",
    "\n",
    "# Pré-processamento dos dados\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # remover acentuação\n",
    "    text = unidecode(text)\n",
    "    # Remover pontuações\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenização\n",
    "    words = word_tokenize(text.lower())\n",
    "    # Remover stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['noticia'] = df['noticia'].apply(preprocess_text)\n",
    "\n",
    "df_treino = df[:210]\n",
    "df_validacao = df[210:]\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Criar modelo de classificação de sentimento\n",
    "vocab_size = 1000\n",
    "embedding_dim = 100\n",
    "max_length = 125\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dense(50, activation='relu'),\n",
    "    keras.layers.Dense(25, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Pré-processar os dados\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df['noticia'])\n",
    "sequences = tokenizer.texts_to_sequences(df['noticia'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "labels = df['choices'].map({'Positiva': 1, 'Negativa': 0, 'Neutra': 0.5})\n",
    "\n",
    "# Treinar o modelo\n",
    "history = model.fit(padded_sequences, labels, epochs=100, validation_split=0.3)\n",
    "\n",
    "# Avaliar o modelo\n",
    "test_sequences = tokenizer.texts_to_sequences(df_validacao['noticia'])\n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "test_labels = df_validacao['choices'].map({'Positiva': 1, 'Negativa': 0, 'Neutra': 0.5})\n",
    "test_loss, test_accuracy = model.evaluate(padded_test_sequences, test_labels)\n",
    "print('Test accuracy:', test_accuracy)\n",
    "\n",
    "# Salvar o modelo\n",
    "model.save('modelo.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## criar uma tabela de predições\n",
    "df_predicoes = df_validacao.copy()\n",
    "df_predicoes['predicao'] = model.predict(padded_test_sequences)\n",
    "df_predicoes['predicao'] = df_predicoes['predicao'].apply(lambda x: 'Positiva' if x > 0.5 else 'Negativa' if x < 0.5 else 'Neutra')\n",
    "df_predicoes\n",
    "\n",
    "##  quantas noticias foram classificadas como positivas, negativas e neutras\n",
    "df_predicoes['predicao'].value_counts()\n",
    "\n",
    "## quantas noticias foram classificadas erradas\n",
    "df_predicoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prever uma noticia\n",
    "df_validacao = df_validacao.reset_index(drop=True)\n",
    "noticia = df_validacao['noticia'][0]\n",
    "noticia = preprocess_text(noticia)\n",
    "noticia = [noticia]\n",
    "noticia = tokenizer.texts_to_sequences(noticia)\n",
    "noticia = pad_sequences(noticia, maxlen=max_length, padding='post', truncating='post')\n",
    "val = model.predict(noticia)[0][0]\n",
    "\n",
    "\n",
    "if val > 0.5:\n",
    "    print('Positiva')\n",
    "elif val < 0.5:\n",
    "    print('Negativa')\n",
    "else:\n",
    "    print('Neutra')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "# Carregar o modelo pré-treinado BERTimbau\n",
    "model = TFBertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss=model.compute_loss, metrics=['accuracy'])\n",
    "\n",
    "# Pré-processar os dados\n",
    "sequences = tokenizer(df['noticia'].tolist(), padding=True, truncation=True, max_length=125)\n",
    "labels = df['choices'].map({'Positiva': 1, 'Negativa': 0, 'Neutra': 0.5})\n",
    "\n",
    "# Treinar o modelo\n",
    "history = model.fit(sequences, labels, epochs=100)\n",
    "\n",
    "# Predict\n",
    "test_sequences = tokenizer(df_validacao['noticia'].tolist(), padding=True, truncation=True, max_length=125)\n",
    "test_labels = df_validacao['choices'].map({'Positiva': 1, 'Negativa': 0, 'Neutra': 0.5})\n",
    "test_loss, test_accuracy = model.evaluate(test_sequences, test_labels)\n",
    "print('Test accuracy:', test_accuracy)\n",
    "\n",
    "# Salvar o modelo\n",
    "model.save_pretrained('bertimbau.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdgd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
