{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file_upload                 294\n",
       "drafts                      294\n",
       "predictions                 294\n",
       "meta                        294\n",
       "created_at                  294\n",
       "updated_at                  294\n",
       "inner_id                    294\n",
       "total_annotations           294\n",
       "cancelled_annotations       294\n",
       "total_predictions           294\n",
       "comment_count               294\n",
       "unresolved_comment_count    294\n",
       "last_comment_updated_at       0\n",
       "project                     294\n",
       "updated_by                  294\n",
       "comment_authors             294\n",
       "completed_by                294\n",
       "was_cancelled               294\n",
       "ground_truth                294\n",
       "created_at                  294\n",
       "updated_at                  294\n",
       "lead_time                   294\n",
       "prediction                  294\n",
       "result_count                294\n",
       "unique_id                   294\n",
       "last_action                   0\n",
       "task                        294\n",
       "project                     294\n",
       "updated_by                  294\n",
       "parent_prediction             0\n",
       "parent_annotation             0\n",
       "last_created_by               0\n",
       "id                          294\n",
       "type                        294\n",
       "origin                      294\n",
       "to_name                     294\n",
       "from_name                   294\n",
       "0                             0\n",
       "choices                     294\n",
       "0                             0\n",
       "ano                         294\n",
       "mes                         294\n",
       "url                         294\n",
       "data                        294\n",
       "titulo                      294\n",
       "noticia                     294\n",
       "data_hora                   294\n",
       "Unnamed: 0                  294\n",
       "qtde_palavras_soja          294\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from unidecode import unidecode\n",
    "## ler arquivo json em pandas\n",
    "df = pd.read_json('/root/projects/personal/comitiva_esperanca/label-studio/data/export/project-1-at-2023-05-07-03-57-20fa509e.json')\n",
    "\n",
    "df = df.drop(['id'], axis=1)\n",
    "## expandir coluna annotations\n",
    "df = pd.concat([df.drop(['annotations'], axis=1), df['annotations'].apply(pd.Series)], axis=1)\n",
    "## expandir coluna 0 e renomear para annotations\n",
    "df = pd.concat([df.drop([0], axis=1), df[0].apply(pd.Series)], axis=1)\n",
    "## expandir coluna result e renomear para result\n",
    "df = pd.concat([df.drop(['result'], axis=1), df['result'].apply(pd.Series)], axis=1)\n",
    "df = df.drop(['id'], axis=1)\n",
    "\n",
    "## expandir coluna 0 e renomear para result\n",
    "df = pd.concat([df.drop([0], axis=1), df[0].apply(pd.Series)], axis=1)\n",
    "## expandir coluna value e renomear para value\n",
    "df = pd.concat([df.drop(['value'], axis=1), df['value'].apply(pd.Series)], axis=1)\n",
    "## dropar choices nulos\n",
    "df = df.dropna(subset=['choices'])\n",
    "## obter choices \n",
    "df['choices'] = df['choices'].apply(lambda x: x[0])\n",
    "\n",
    "## expandir coluna data\n",
    "df = pd.concat([df.drop(['data'], axis=1), df['data'].apply(pd.Series)], axis=1)\n",
    "\n",
    "df_noticia_original = df.copy()\n",
    "\n",
    "\n",
    "padrao_data_cepea = r\"Cepea, \\d{2}/\\d{2}/\\d{4} - \"\n",
    "df['noticia'] = df['noticia'].apply(lambda x: re.sub(padrao_data_cepea, '', x))\n",
    "\n",
    "## remover a palavra 'cepea' das noticias\n",
    "padrao_cepea = r\"Cepea\"\n",
    "df['noticia'] = df['noticia'].apply(lambda x: re.sub(padrao_cepea, '', x, flags=re.IGNORECASE))\n",
    "\n",
    "## remover numeros das noticias\n",
    "padrao_numeros = r'[0-9]+'\n",
    "df['noticia'] = df['noticia'].apply(lambda x: re.sub(padrao_numeros, '', x))\n",
    "\n",
    "## noticia que contem a palavra 'soja'\n",
    "df = df[df['titulo'].str.contains('soja', flags=re.IGNORECASE)]\n",
    "\n",
    "## remover noticias com choice 'desclassificar'\n",
    "df = df[df['choices'] != 'Desclassificar']\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar apenas as colunas necessárias\n",
    "columns_to_select = ['id', 'data', 'noticia', 'titulo', 'choices', 'unique_id']\n",
    "\n",
    "df = df[columns_to_select]\n",
    "df.dropna(subset=['noticia'])\n",
    "\n",
    "# Pré-processamento dos dados\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # remover acentuação\n",
    "    text = unidecode(text)\n",
    "    # Remover pontuações\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenização\n",
    "    words = word_tokenize(text.lower())\n",
    "    # Remover stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# df['noticia'] = df['noticia'].apply(preprocess_text)\n",
    "\n",
    "df_treino = df[:210]\n",
    "df_validacao = df[210:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/projects/personal/comitiva_esperanca/.python_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.28.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 119547\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer  # Or BertTokenizer\n",
    "from transformers import AutoModelForPreTraining  # Or BertForPreTraining for loading pretraining heads\n",
    "from transformers import AutoModel  # or BertModel, for BERT without pretraining heads\n",
    "import numpy as np\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "# Carregar o modelo pré-treinado BERTimbau\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "modelo_soja = model\n",
    "\n",
    "modelo_soja.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIABertClassifier(nn.Module):\n",
    "    def __init__(self,model,num_labels):\n",
    "        super(LIABertClassifier,self).__init__()\n",
    "        self.bert = model\n",
    "        self.config = model.config\n",
    "        self.num_labels = num_labels\n",
    "        self.cls = nn.Linear(self.config.hidden_size,200)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.cls2 = nn.Linear(200,num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        ) ->Tuple[torch.Tensor]:\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0][:,0,:]\n",
    "        prediction = self.cls(sequence_output)\n",
    "        prediction = self.dropout(prediction)\n",
    "        prediction = self.cls2(prediction)\n",
    "        return prediction\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(', // – Os preços da soja estão em alta no Brasil. O Indicador ESALQ/BM&FBovespa da soja Paranaguá (PR) subiu ,% entre  e  de março, a R$ ,/sc de  kg nessa sexta-feira,  – na quarta-feira, , o Indicador atingiu R$ ,/sc, o maior valor nominal da série histórica do , iniciada em março/. Quanto ao Indicador /ESALQ Paraná, foi de R$ ,/sc de  kg na sexta-feira, , ,% superior ao dia . Segundo pesquisadores do , as elevações no Brasil estão atreladas à combinação de firmes demandas externa e doméstica, da alta nos preços futuros e do dólar elevado. Além disso, para combater o avanço do coronavírus, o governo argentino limitou o movimento nos portos do país, cenário que pode favorecer as vendas brasileiras de soja e derivados. Fonte:  – ', 1)\n",
      "(', // -\\xa0Os preços internos da soja seguiram em alta nos últimos dias, mesmo com o avanço da colheita no Brasil. A sustentação veio do mercado internacional. Além do surpreendente aumento da demanda chinesa pela soja, novas estimativas apontam menor produção do grão na Argentina. Entre  e  de abril, o Indicador /ESALQ (média de cinco regiões do Paraná) da soja em grão teve alta de ,%, fechando a R$ ,/sc de  kg nessa segunda-feira. Para o Indicador ESALQ/BM&FBovespa (produto posto porto de Paranaguá) houve também alta de ,% nos últimos sete dias, fechando a R$ ,/sc de  kg nessa segunda-feira, . ', 1)\n",
      "(', // – O ritmo das exportações de soja e derivados diminuiu nesta semana, devido à dificuldade de embarque. Alguns traders consultados pelo  indicam que as chuvas ocorridas nas últimas semanas atrasaram a programação de escoamento nos principais portos brasileiros: Santos (SP), Paranaguá (PR) e Rio Grande (RS). Esse cenário diminuiu as cotas nos armazéns portuários para tradings que pretendiam enviar novos lotes para esses portos. As exportações foram limitadas também pelo atraso de navios, especialmente em Santos. Neste caso, os lotes que deveriam ser escoados nas duas últimas semanas foram prorrogados para a segunda quinzena deste mês. Na região do Matopiba, de acordo com levantamento do , precipitações seguem atrasando a colheita e limitando as comercializações. Quanto aos preços, o Indicador ESALQ/BM&FBovespa da soja Paranaguá recuou ,%, a R$ ,/saca de  kg no dia . No mesmo comparativo, o Indicador /ESALQ Paraná teve baixa de ,%, a R$ ,/sc de  kg nessa sexta. Fonte:  – www..esalq.usp.br', 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 01:43:23.556754: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-07 01:43:25.331722: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/root/projects/personal/comitiva_esperanca/.python_env/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ytrain_global = np.array(df_treino['choices'])\n",
    "\n",
    "## transformar as classes em 0 = negativo, 1 = positivo, 2 = neutro\n",
    "ytrain_global = np.where(ytrain_global == 'Positiva', 1, ytrain_global)\n",
    "ytrain_global = np.where(ytrain_global == 'Negativa', 0, ytrain_global)\n",
    "ytrain_global = np.where(ytrain_global == 'Neutra', 2, ytrain_global)\n",
    "\n",
    "ytrain_global = ytrain_global.astype(int)\n",
    "\n",
    "xtrain_global = np.array(df_treino['noticia'])\n",
    "\n",
    "xtrain, xval, ytrain, yval = train_test_split(xtrain_global, ytrain_global, test_size=0.2, random_state=42)\n",
    "\n",
    "for v in zip(xtrain[:3],ytrain[:3]):\n",
    "    print(v)\n",
    "\n",
    "train_encodings = tokenizer(xtrain.tolist(), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "val_encodings = tokenizer(xval.tolist(), truncation=True, padding=True,max_length=512, return_tensors='pt')\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], torch.tensor(ytrain))\n",
    "val_dataset = torch.utils.data.TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], torch.tensor(yval))\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model = LIABertClassifier(modelo_soja, 2)\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def train_epoch(model, train_loader, optim):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    acc = []\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        losses.append(loss.item())\n",
    "        acc.append(accuracy_score(labels.cpu().numpy(), outputs.argmax(1).cpu().numpy()))\n",
    "    return np.array(losses).mean(), np.array(acc).mean()\n",
    "\n",
    "def val_epoch(model, val_loader):\n",
    "\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    acc = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "            acc.append(accuracy_score(labels.cpu().numpy(), outputs.argmax(1).cpu().numpy()))\n",
    "    return np.array(losses).mean(), np.array(acc).mean()\n",
    "\n",
    "for epoch in range(5):\n",
    "    loss, acc = train_epoch(model, train_loader, optim)\n",
    "    print(f'Epoch {epoch} train loss {loss} train accuracy {acc}')\n",
    "    loss, acc = val_epoch(model, val_loader)\n",
    "    print(f'Epoch {epoch} val loss {loss} val accuracy {acc}')\n",
    "\n",
    "## mostrar a matriz de confusão, precisão, recall e f1-score\n",
    "y_pred = []\n",
    "y_true = []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        y_pred.extend(outputs.argmax(1).cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdgd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
