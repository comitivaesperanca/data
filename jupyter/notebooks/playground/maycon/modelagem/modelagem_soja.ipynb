{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from unidecode import unidecode\n",
    "## ler arquivo json em pandas\n",
    "df = pd.read_json('/root/projects/personal/comitiva_esperanca/label-studio/data/export/project-1-at-2023-05-07-03-57-20fa509e.json')\n",
    "\n",
    "df = df.drop(['id'], axis=1)\n",
    "## expandir coluna annotations\n",
    "df = pd.concat([df.drop(['annotations'], axis=1), df['annotations'].apply(pd.Series)], axis=1)\n",
    "## expandir coluna 0 e renomear para annotations\n",
    "df = pd.concat([df.drop([0], axis=1), df[0].apply(pd.Series)], axis=1)\n",
    "## expandir coluna result e renomear para result\n",
    "df = pd.concat([df.drop(['result'], axis=1), df['result'].apply(pd.Series)], axis=1)\n",
    "df = df.drop(['id'], axis=1)\n",
    "\n",
    "## expandir coluna 0 e renomear para result\n",
    "df = pd.concat([df.drop([0], axis=1), df[0].apply(pd.Series)], axis=1)\n",
    "## expandir coluna value e renomear para value\n",
    "df = pd.concat([df.drop(['value'], axis=1), df['value'].apply(pd.Series)], axis=1)\n",
    "## dropar choices nulos\n",
    "df = df.dropna(subset=['choices'])\n",
    "## obter choices \n",
    "df['choices'] = df['choices'].apply(lambda x: x[0])\n",
    "\n",
    "## expandir coluna data\n",
    "df = pd.concat([df.drop(['data'], axis=1), df['data'].apply(pd.Series)], axis=1)\n",
    "\n",
    "df_noticia_original = df.copy()\n",
    "\n",
    "\n",
    "padrao_data_cepea = r\"Cepea, \\d{2}/\\d{2}/\\d{4} - \"\n",
    "df['noticia'] = df['noticia'].apply(lambda x: re.sub(padrao_data_cepea, '', x))\n",
    "\n",
    "## remover a palavra 'cepea' das noticias\n",
    "padrao_cepea = r\"Cepea\"\n",
    "df['noticia'] = df['noticia'].apply(lambda x: re.sub(padrao_cepea, '', x, flags=re.IGNORECASE))\n",
    "\n",
    "## remover numeros das noticias\n",
    "padrao_numeros = r'[0-9]+'\n",
    "df['noticia'] = df['noticia'].apply(lambda x: re.sub(padrao_numeros, '', x))\n",
    "\n",
    "## noticia que contem a palavra 'soja'\n",
    "df = df[df['titulo'].str.contains('soja', flags=re.IGNORECASE)]\n",
    "\n",
    "## remover noticias com choice 'desclassificar'\n",
    "df = df[df['choices'] != 'Desclassificar']\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar apenas as colunas necessárias\n",
    "columns_to_select = ['id', 'data', 'noticia', 'titulo', 'choices', 'unique_id']\n",
    "\n",
    "df = df[columns_to_select]\n",
    "df.dropna(subset=['noticia'])\n",
    "\n",
    "# Pré-processamento dos dados\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # remover acentuação\n",
    "    text = unidecode(text)\n",
    "    # Remover pontuações\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenização\n",
    "    words = word_tokenize(text.lower())\n",
    "    # Remover stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['noticia'] = df['noticia'].apply(preprocess_text)\n",
    "df_treino = df[:208]\n",
    "df_validacao = df[208:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer  # Or BertTokenizer\n",
    "from transformers import AutoModelForPreTraining  # Or BertForPreTraining for loading pretraining heads\n",
    "from transformers import AutoModel  # or BertModel, for BERT without pretraining heads\n",
    "import numpy as np\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "# Carregar o modelo pré-treinado BERTimbau\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "modelo_soja = model\n",
    "\n",
    "modelo_soja.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIABertClassifier(nn.Module):\n",
    "    def __init__(self,model,num_labels):\n",
    "        super(LIABertClassifier,self).__init__()\n",
    "        self.bert = model\n",
    "        self.config = model.config\n",
    "        self.num_labels = num_labels\n",
    "        self.cls = nn.Linear(self.config.hidden_size,200)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.cls2 = nn.Linear(200,num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        ) ->Tuple[torch.Tensor]:\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0][:,0,:]\n",
    "        prediction = self.cls(sequence_output)\n",
    "        prediction = self.dropout(prediction)\n",
    "        prediction = self.cls2(prediction)\n",
    "        return prediction\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_scheduler\n",
    "import sklearn.model_selection as model_selection\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# Map string labels to integer labels\n",
    "label_map = {'Negativa': 0, 'Positiva': 1, 'Neutra': 2}\n",
    "ytrain_global = np.array([label_map[label] for label in df_treino['choices']])\n",
    "xtrain_global = np.array(df_treino['noticia'])\n",
    "\n",
    "\n",
    "\n",
    "xtrain, xval, ytrain, yval = model_selection.train_test_split(xtrain_global, ytrain_global, test_size=0.30, random_state=42,shuffle=True)\n",
    "\n",
    "train_encodings = tokenizer(xtrain.tolist(), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "val_encodings = tokenizer(xval.tolist(), truncation=True, padding=True,max_length=512, return_tensors='pt')\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        return (item,label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = MyDataset(train_encodings, ytrain)\n",
    "val_dataset = MyDataset(val_encodings, yval)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model = LIABertClassifier(modelo_soja, 3)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "dl_train = DataLoader(train_dataset,batch_size=8)\n",
    "dl_eval  = DataLoader(val_dataset,batch_size=8)\n",
    "x,y = next(iter(dl_train))\n",
    "\n",
    "batch = {k: v.to(device) for k, v in x.items()}\n",
    "model.to(device)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=0.000050)\n",
    "\n",
    "loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optim,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_epochs*len(dl_train)\n",
    ")\n",
    "\n",
    "# scheduler = OneCycleLR(optim, max_lr=max_lr, total_steps=total_steps,\n",
    "#                        div_factor=div_factor, pct_start=pct_start)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    lepochs = []\n",
    "    for batch,y in dl_train:\n",
    "        model.train()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        y = y.to(device)\n",
    "        outputs = model(**batch)\n",
    "        loss = loss_fct(outputs,y)\n",
    "        lepochs.append(loss.cpu().item())\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "        optim.zero_grad()\n",
    "        current_lr = optim.param_groups[0]['lr']\n",
    "\n",
    "        print(f\"Epoch {epoch}, Loss={loss:.4f}, LR={current_lr:.6f}\")\n",
    "\n",
    "model.eval()\n",
    "ytrue = []\n",
    "ypred = []\n",
    "for batch,y in dl_eval:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    y = y.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "        ytrue.extend(y.cpu().numpy().tolist())\n",
    "        ypred.extend(outputs.argmax(axis=1).cpu().numpy().tolist())\n",
    "\n",
    "print(classification_report(ytrue,ypred))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdgd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
