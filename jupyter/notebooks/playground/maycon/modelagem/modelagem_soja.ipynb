{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from unidecode import unidecode\n",
    "## ler arquivo json em pandas\n",
    "df = pd.read_json('/home/mfelipemota/projects/pantanaldev/data/label-studio/data/export/project-1-at-2023-05-07-03-57-20fa509e.json')\n",
    "\n",
    "df = df.drop(['id'], axis=1)\n",
    "## expandir coluna annotations\n",
    "df = pd.concat([df.drop(['annotations'], axis=1), df['annotations'].apply(pd.Series)], axis=1)\n",
    "## expandir coluna 0 e renomear para annotations\n",
    "df = pd.concat([df.drop([0], axis=1), df[0].apply(pd.Series)], axis=1)\n",
    "## expandir coluna result e renomear para result\n",
    "df = pd.concat([df.drop(['result'], axis=1), df['result'].apply(pd.Series)], axis=1)\n",
    "df = df.drop(['id'], axis=1)\n",
    "\n",
    "## expandir coluna 0 e renomear para result\n",
    "df = pd.concat([df.drop([0], axis=1), df[0].apply(pd.Series)], axis=1)\n",
    "## expandir coluna value e renomear para value\n",
    "df = pd.concat([df.drop(['value'], axis=1), df['value'].apply(pd.Series)], axis=1)\n",
    "## dropar choices nulos\n",
    "df = df.dropna(subset=['choices'])\n",
    "## obter choices \n",
    "df['choices'] = df['choices'].apply(lambda x: x[0])\n",
    "\n",
    "## expandir coluna data\n",
    "df = pd.concat([df.drop(['data'], axis=1), df['data'].apply(pd.Series)], axis=1)\n",
    "\n",
    "df_noticia_original = df.copy()\n",
    "\n",
    "\n",
    "padrao_data_cepea = r\"Cepea, \\d{2}/\\d{2}/\\d{4} - \"\n",
    "df['noticia'] = df['noticia'].apply(lambda x: re.sub(padrao_data_cepea, '', x))\n",
    "\n",
    "## remover a palavra 'cepea' das noticias\n",
    "padrao_cepea = r\"Cepea\"\n",
    "df['noticia'] = df['noticia'].apply(lambda x: re.sub(padrao_cepea, '', x, flags=re.IGNORECASE))\n",
    "\n",
    "## remover numeros das noticias\n",
    "padrao_numeros = r'[0-9]+'\n",
    "df['noticia'] = df['noticia'].apply(lambda x: re.sub(padrao_numeros, '', x))\n",
    "\n",
    "## noticia que contem a palavra 'soja'\n",
    "df = df[df['titulo'].str.contains('soja', flags=re.IGNORECASE)]\n",
    "\n",
    "## remover noticias com choice 'desclassificar'\n",
    "df = df[df['choices'] != 'Desclassificar']\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar apenas as colunas necessárias\n",
    "columns_to_select = ['id', 'data', 'noticia', 'titulo', 'choices', 'unique_id']\n",
    "\n",
    "df = df[columns_to_select]\n",
    "df.dropna(subset=['noticia'])\n",
    "\n",
    "# Pré-processamento dos dados\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # remover acentuação\n",
    "    text = unidecode(text)\n",
    "    # Remover pontuações\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenização\n",
    "    words = word_tokenize(text.lower())\n",
    "    # Remover stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['noticia'] = df['noticia'].apply(preprocess_text)\n",
    "df_treino = df[:208]\n",
    "df_validacao = df[208:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mfelipemota/projects/pantanaldev/data/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.28.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 119547\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer  # Or BertTokenizer\n",
    "from transformers import AutoModelForPreTraining  # Or BertForPreTraining for loading pretraining heads\n",
    "from transformers import AutoModel  # or BertModel, for BERT without pretraining heads\n",
    "import numpy as np\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "# Carregar o modelo pré-treinado BERTimbau\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "modelo_soja = model\n",
    "\n",
    "modelo_soja.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIABertClassifier(nn.Module):\n",
    "    def __init__(self,model,num_labels):\n",
    "        super(LIABertClassifier,self).__init__()\n",
    "        self.bert = model\n",
    "        self.config = model.config\n",
    "        self.num_labels = num_labels\n",
    "        self.cls = nn.Linear(self.config.hidden_size,200)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.cls2 = nn.Linear(200,num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        ) ->Tuple[torch.Tensor]:\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0][:,0,:]\n",
    "        prediction = self.cls(sequence_output)\n",
    "        prediction = self.dropout(prediction)\n",
    "        prediction = self.cls2(prediction)\n",
    "        return prediction\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(', // – Os preços da soja estão em alta no Brasil. O Indicador ESALQ/BM&FBovespa da soja Paranaguá (PR) subiu ,% entre  e  de março, a R$ ,/sc de  kg nessa sexta-feira,  – na quarta-feira, , o Indicador atingiu R$ ,/sc, o maior valor nominal da série histórica do , iniciada em março/. Quanto ao Indicador /ESALQ Paraná, foi de R$ ,/sc de  kg na sexta-feira, , ,% superior ao dia . Segundo pesquisadores do , as elevações no Brasil estão atreladas à combinação de firmes demandas externa e doméstica, da alta nos preços futuros e do dólar elevado. Além disso, para combater o avanço do coronavírus, o governo argentino limitou o movimento nos portos do país, cenário que pode favorecer as vendas brasileiras de soja e derivados. Fonte:  – ', 1)\n",
      "(', // -\\xa0Os preços internos da soja seguiram em alta nos últimos dias, mesmo com o avanço da colheita no Brasil. A sustentação veio do mercado internacional. Além do surpreendente aumento da demanda chinesa pela soja, novas estimativas apontam menor produção do grão na Argentina. Entre  e  de abril, o Indicador /ESALQ (média de cinco regiões do Paraná) da soja em grão teve alta de ,%, fechando a R$ ,/sc de  kg nessa segunda-feira. Para o Indicador ESALQ/BM&FBovespa (produto posto porto de Paranaguá) houve também alta de ,% nos últimos sete dias, fechando a R$ ,/sc de  kg nessa segunda-feira, . ', 1)\n",
      "(', // – O ritmo das exportações de soja e derivados diminuiu nesta semana, devido à dificuldade de embarque. Alguns traders consultados pelo  indicam que as chuvas ocorridas nas últimas semanas atrasaram a programação de escoamento nos principais portos brasileiros: Santos (SP), Paranaguá (PR) e Rio Grande (RS). Esse cenário diminuiu as cotas nos armazéns portuários para tradings que pretendiam enviar novos lotes para esses portos. As exportações foram limitadas também pelo atraso de navios, especialmente em Santos. Neste caso, os lotes que deveriam ser escoados nas duas últimas semanas foram prorrogados para a segunda quinzena deste mês. Na região do Matopiba, de acordo com levantamento do , precipitações seguem atrasando a colheita e limitando as comercializações. Quanto aos preços, o Indicador ESALQ/BM&FBovespa da soja Paranaguá recuou ,%, a R$ ,/saca de  kg no dia . No mesmo comparativo, o Indicador /ESALQ Paraná teve baixa de ,%, a R$ ,/sc de  kg nessa sexta. Fonte:  – www..esalq.usp.br', 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 01:50:41.718216: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-07 01:50:42.149154: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-07 01:50:43.319144: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/mfelipemota/projects/pantanaldev/data/.venv/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 2 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(losses)\u001b[39m.\u001b[39mmean(), np\u001b[39m.\u001b[39marray(acc)\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     72\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[0;32m---> 73\u001b[0m     loss, acc \u001b[39m=\u001b[39m train_epoch(model, train_loader, optim)\n\u001b[1;32m     74\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m train loss \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m train accuracy \u001b[39m\u001b[39m{\u001b[39;00macc\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     75\u001b[0m     loss, acc \u001b[39m=\u001b[39m val_epoch(model, val_loader)\n",
      "Cell \u001b[0;32mIn[5], line 49\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, optim)\u001b[0m\n\u001b[1;32m     47\u001b[0m labels \u001b[39m=\u001b[39m batch[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     48\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids, attention_mask\u001b[39m=\u001b[39mattention_mask)\n\u001b[0;32m---> 49\u001b[0m loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mCrossEntropyLoss()(outputs, labels)\n\u001b[1;32m     50\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     51\u001b[0m optim\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/projects/pantanaldev/data/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/pantanaldev/data/.venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/projects/pantanaldev/data/.venv/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 2 is out of bounds."
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_scheduler\n",
    "import sklearn.model_selection as model_selection\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# Map string labels to integer labels\n",
    "label_map = {'Negativa': 0, 'Positiva': 1, 'Neutra': 2}\n",
    "ytrain_global = np.array([label_map[label] for label in df_treino['choices']])\n",
    "xtrain_global = np.array(df_treino['noticia'])\n",
    "\n",
    "\n",
    "\n",
    "xtrain, xval, ytrain, yval = model_selection.train_test_split(xtrain_global, ytrain_global, test_size=0.30, random_state=42,shuffle=True)\n",
    "\n",
    "train_encodings = tokenizer(xtrain.tolist(), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "val_encodings = tokenizer(xval.tolist(), truncation=True, padding=True,max_length=512, return_tensors='pt')\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        return (item,label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = MyDataset(train_encodings, ytrain)\n",
    "val_dataset = MyDataset(val_encodings, yval)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model = LIABertClassifier(modelo_soja, 3)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "batch = {k: v.to(device) for k, v in x.items()}\n",
    "model.to(device)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=0.000050)\n",
    "\n",
    "loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optim,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_epochs*len(dl_train)\n",
    ")\n",
    "\n",
    "# scheduler = OneCycleLR(optim, max_lr=max_lr, total_steps=total_steps,\n",
    "#                        div_factor=div_factor, pct_start=pct_start)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    lepochs = []\n",
    "    for batch,y in dl_train:\n",
    "        model.train()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        y = y.to(device)\n",
    "        outputs = model(**batch)\n",
    "        loss = loss_fct(outputs,y)\n",
    "        lepochs.append(loss.cpu().item())\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "        optim.zero_grad()\n",
    "        current_lr = optim.param_groups[0]['lr']\n",
    "\n",
    "        print(f\"Epoch {epoch}, Loss={loss:.4f}, LR={current_lr:.6f}\")\n",
    "\n",
    "model.eval()\n",
    "ytrue = []\n",
    "ypred = []\n",
    "for batch,y in dl_eval:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    y = y.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "        ytrue.extend(y.cpu().numpy().tolist())\n",
    "        ypred.extend(outputs.argmax(axis=1).cpu().numpy().tolist())\n",
    "\n",
    "for epoch in range(5):\n",
    "    loss, acc = train_epoch(model, train_loader, optim)\n",
    "    print(f'Epoch {epoch} train loss {loss} train accuracy {acc}')\n",
    "    loss, acc = val_epoch(model, val_loader)\n",
    "    print(f'Epoch {epoch} val loss {loss} val accuracy {acc}')\n",
    "\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdgd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
