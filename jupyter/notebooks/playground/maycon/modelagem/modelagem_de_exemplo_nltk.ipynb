{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File C:\\Users\\mayco\\Documents\\projetos\\data-pantanaldev\\label-studio\\data\\export\\project-1-at-2023-04-30-05-08-956f4e3c.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39munidecode\u001b[39;00m \u001b[39mimport\u001b[39;00m unidecode\n\u001b[1;32m     19\u001b[0m \u001b[39m## ler arquivo json em pandas\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_json(\u001b[39m'\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mmayco\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mDocuments\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mprojetos\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mdata-pantanaldev\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mlabel-studio\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mdata\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mexport\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mproject-1-at-2023-04-30-05-08-956f4e3c.json\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     22\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[39m## expandir coluna annotations\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/personal/comitiva_esperanca/.python_env/lib/python3.8/site-packages/pandas/io/json/_json.py:760\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[39mif\u001b[39;00m convert_axes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m orient \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtable\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    758\u001b[0m     convert_axes \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 760\u001b[0m json_reader \u001b[39m=\u001b[39m JsonReader(\n\u001b[1;32m    761\u001b[0m     path_or_buf,\n\u001b[1;32m    762\u001b[0m     orient\u001b[39m=\u001b[39;49morient,\n\u001b[1;32m    763\u001b[0m     typ\u001b[39m=\u001b[39;49mtyp,\n\u001b[1;32m    764\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    765\u001b[0m     convert_axes\u001b[39m=\u001b[39;49mconvert_axes,\n\u001b[1;32m    766\u001b[0m     convert_dates\u001b[39m=\u001b[39;49mconvert_dates,\n\u001b[1;32m    767\u001b[0m     keep_default_dates\u001b[39m=\u001b[39;49mkeep_default_dates,\n\u001b[1;32m    768\u001b[0m     precise_float\u001b[39m=\u001b[39;49mprecise_float,\n\u001b[1;32m    769\u001b[0m     date_unit\u001b[39m=\u001b[39;49mdate_unit,\n\u001b[1;32m    770\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    771\u001b[0m     lines\u001b[39m=\u001b[39;49mlines,\n\u001b[1;32m    772\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m    773\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    774\u001b[0m     nrows\u001b[39m=\u001b[39;49mnrows,\n\u001b[1;32m    775\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    776\u001b[0m     encoding_errors\u001b[39m=\u001b[39;49mencoding_errors,\n\u001b[1;32m    777\u001b[0m     dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,\n\u001b[1;32m    778\u001b[0m     engine\u001b[39m=\u001b[39;49mengine,\n\u001b[1;32m    779\u001b[0m )\n\u001b[1;32m    781\u001b[0m \u001b[39mif\u001b[39;00m chunksize:\n\u001b[1;32m    782\u001b[0m     \u001b[39mreturn\u001b[39;00m json_reader\n",
      "File \u001b[0;32m~/projects/personal/comitiva_esperanca/.python_env/lib/python3.8/site-packages/pandas/io/json/_json.py:861\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m filepath_or_buffer\n\u001b[1;32m    860\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mujson\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 861\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data_from_filepath(filepath_or_buffer)\n\u001b[1;32m    862\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[0;32m~/projects/personal/comitiva_esperanca/.python_env/lib/python3.8/site-packages/pandas/io/json/_json.py:917\u001b[0m, in \u001b[0;36mJsonReader._get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    909\u001b[0m     filepath_or_buffer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n\u001b[1;32m    910\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[1;32m    911\u001b[0m     \u001b[39misinstance\u001b[39m(filepath_or_buffer, \u001b[39mstr\u001b[39m)\n\u001b[1;32m    912\u001b[0m     \u001b[39mand\u001b[39;00m filepath_or_buffer\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mendswith(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[1;32m    916\u001b[0m ):\n\u001b[0;32m--> 917\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFile \u001b[39m\u001b[39m{\u001b[39;00mfilepath_or_buffer\u001b[39m}\u001b[39;00m\u001b[39m does not exist\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    919\u001b[0m \u001b[39mreturn\u001b[39;00m filepath_or_buffer\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File C:\\Users\\mayco\\Documents\\projetos\\data-pantanaldev\\label-studio\\data\\export\\project-1-at-2023-04-30-05-08-956f4e3c.json does not exist"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from unidecode import unidecode\n",
    "## ler arquivo json em pandas\n",
    "df = pd.read_json('C:\\\\Users\\\\mayco\\\\Documents\\\\projetos\\\\data-pantanaldev\\\\label-studio\\\\data\\\\export\\\\project-1-at-2023-04-30-05-08-956f4e3c.json')\n",
    "\n",
    "df = df.drop(['id'], axis=1)\n",
    "## expandir coluna annotations\n",
    "df = pd.concat([df.drop(['annotations'], axis=1), df['annotations'].apply(pd.Series)], axis=1)\n",
    "## expandir coluna 0 e renomear para annotations\n",
    "df = pd.concat([df.drop([0], axis=1), df[0].apply(pd.Series)], axis=1)\n",
    "## expandir coluna result e renomear para result\n",
    "df = pd.concat([df.drop(['result'], axis=1), df['result'].apply(pd.Series)], axis=1)\n",
    "df = df.drop(['id'], axis=1)\n",
    "\n",
    "## expandir coluna 0 e renomear para result\n",
    "df = pd.concat([df.drop([0], axis=1), df[0].apply(pd.Series)], axis=1)\n",
    "## expandir coluna value e renomear para value\n",
    "df = pd.concat([df.drop(['value'], axis=1), df['value'].apply(pd.Series)], axis=1)\n",
    "## dropar choices nulos\n",
    "df = df.dropna(subset=['choices'])\n",
    "## obter choices \n",
    "df['choices'] = df['choices'].apply(lambda x: x[0])\n",
    "\n",
    "## expandir coluna data\n",
    "df = pd.concat([df.drop(['data'], axis=1), df['data'].apply(pd.Series)], axis=1)\n",
    "\n",
    "df_noticia_original = df.copy()\n",
    "\n",
    "\n",
    "padrao_data_cepea = r\"Cepea, \\d{2}/\\d{2}/\\d{4} - \"\n",
    "df['noticia'] = df['noticia'].apply(lambda x: re.sub(padrao_data_cepea, '', x))\n",
    "\n",
    "## remover a palavra 'cepea' das noticias\n",
    "padrao_cepea = r\"Cepea\"\n",
    "df['noticia'] = df['noticia'].apply(lambda x: re.sub(padrao_cepea, '', x, flags=re.IGNORECASE))\n",
    "\n",
    "## remover numeros das noticias\n",
    "padrao_numeros = r'[0-9]+'\n",
    "df['noticia'] = df['noticia'].apply(lambda x: re.sub(padrao_numeros, '', x))\n",
    "\n",
    "## noticia que contem a palavra 'soja'\n",
    "df = df[df['titulo'].str.contains('soja', flags=re.IGNORECASE)]\n",
    "\n",
    "## remover noticias com choice 'desclassificar'\n",
    "df = df[df['choices'] != 'Desclassificar']\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar apenas as colunas necess√°rias\n",
    "columns_to_select = ['id', 'data', 'noticia', 'titulo', 'choices', 'unique_id']\n",
    "\n",
    "df = df[columns_to_select]\n",
    "df.dropna(subset=['noticia'])\n",
    "\n",
    "# Pr√©-processamento dos dados\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # remover acentua√ß√£o\n",
    "    text = unidecode(text)\n",
    "    # Remover pontua√ß√µes\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokeniza√ß√£o\n",
    "    words = word_tokenize(text.lower())\n",
    "    # Remover stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['noticia'] = df['noticia'].apply(preprocess_text)\n",
    "\n",
    "df_treino = df[:188]\n",
    "df_validacao = df[188:]\n",
    "\n",
    "\n",
    "# Criar modelo de classifica√ß√£o\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_treino['noticia'], df_treino['choices'], test_size=0.2)\n",
    "\n",
    "# Definir pipeline com CountVectorizer e MultinomialNB\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', None)\n",
    "])\n",
    "\n",
    "# Definir grade de par√¢metros a serem testados\n",
    "parameters = {'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "              'classifier': [MultinomialNB(), DecisionTreeClassifier()],\n",
    "              }\n",
    "\n",
    "# Criar objeto GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, scoring='accuracy')\n",
    "\n",
    "# Treinar modelos em diferentes combina√ß√µes de hiperpar√¢metros\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Selecionar o melhor modelo com base na pontua√ß√£o de valida√ß√£o cruzada\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_probs = best_model.predict_proba(X_test)\n",
    "\n",
    "## avalia o modelo, gerando report, matriz de confus√£o e acur√°cia\n",
    "print(best_model.score(X_test, y_test))\n",
    "\n",
    "## printa o nome do melhor modelo escolhido pelo gridsearch\n",
    "print(best_model.named_steps['classifier'])\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "## salva o modelo\n",
    "\n",
    "import pickle\n",
    "pickle.dump(best_model, open('modelo.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## treina no conjunto de valida√ß√£o\n",
    "\n",
    "model = pickle.load(open('modelo.pkl', 'rb'))\n",
    "\n",
    "df_validacao = df_validacao.dropna(subset=['noticia']).reset_index(drop=True)\n",
    "y_pred = model.predict_proba(df_validacao['noticia'])\n",
    "y_pred_class = model.predict(df_validacao['noticia'])\n",
    "\n",
    "print(classification_report(df_validacao['choices'], y_pred_class))\n",
    "## eliminar o e+01 do valor numerico em pandas\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "df_predict = pd.DataFrame(y_pred, columns=model.classes_)\n",
    "df_predict['predict_final'] = df_predict.idxmax(axis=1)\n",
    "print(df_predict.columns)\n",
    "\n",
    "df_pred_concatenado = pd.concat([df_validacao, df_predict], axis=1)\n",
    "df_pred_concatenado[['id', 'data', 'titulo', 'noticia', 'choices', 'predict_final', 'Neutra', 'Negativa', 'Positiva']] #.to_excel('validacao.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_noticia = \"Fim da greve dos caminhoneiros colabora para o pre√ßo da soja no Brasil a subir\"\n",
    "texto_noticia = preprocess_text(texto_noticia)\n",
    "print(texto_noticia)\n",
    "\n",
    "\n",
    "y_pred = model.predict_proba([texto_noticia])\n",
    "y_pred_class = model.predict([texto_noticia])\n",
    "\n",
    "print(model.classes_)\n",
    "print(y_pred)\n",
    "print(y_pred_class)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdgd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
